{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd48a22-2567-4638-9d23-c507eb6db083",
   "metadata": {},
   "source": [
    "### Advanced Python AI and ML Tools - Assignment 1\n",
    "\n",
    "__Group Members:__\n",
    "1) Aanal Patel - C0910376\n",
    "2) Bimal Shresta - C0919385\n",
    "3) Danilo Diaz - C0889539\n",
    "4) Ernie Sumoso - C0881591"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d8930-578d-4157-b1c5-368a73ef4b63",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Index\n",
    "- __Step 1. Dataset Description (web scrapped)__\n",
    "- __Step 2. Data Wrangling (cleaning, formatting, structuring, validating)__\n",
    "    - __Step 9. NLP techniques (data cleaning, stopword and puctuation removal, tokenizing, stemming, and lemmatization)__\n",
    "- __Step 3. Plotting methods for distribution__\n",
    "- __Step 4. Pandas profiling for EDA (exploratory data analysis)__\n",
    "- __Step 5. Encoding methods, creating new numerical columns__\n",
    "- __Step 6. Outlier identification (with boxplots and IQR)__\n",
    "- __Step 7. Addressing outliers with Quantile-based flooring and capping, Trimming, and Log Transformation__\n",
    "- __Step 8. Unsupervised learning methods__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d3e558-267b-4d55-8b33-4c33c6e77d86",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1. Dataset Description (web scrapped)\n",
    "\n",
    "(Bimal add a description of what you did to web scrap the data here, what is the source and what were your steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a9a780-fd82-442c-bce2-85cd9bc8e1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>job_location</th>\n",
       "      <th>post</th>\n",
       "      <th>job_type</th>\n",
       "      <th>job_desc</th>\n",
       "      <th>company_qns</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>job_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>Level 2/3 Support Engineer</td>\n",
       "      <td>Fuse Technology Pty Ltd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>Help Desk &amp; IT Support (Information &amp; Communic...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>The opportunityAs part of our exciting growth ...</td>\n",
       "      <td>Which of the following statements best describ...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73930150?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>NIGHT SHIFT WAREHOUSE TEAM LEADER WANTED WETHE...</td>\n",
       "      <td>Labourforce</td>\n",
       "      <td>$47 per hour + penalties</td>\n",
       "      <td>Wetherill Park, Sydney NSW</td>\n",
       "      <td>Warehousing, Storage &amp; Distribution (Manufactu...</td>\n",
       "      <td>Contract/Temp</td>\n",
       "      <td>Our client is one of Australia's leading Manuf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73870879?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>Casual Retail Assistant</td>\n",
       "      <td>Independent Living Specialists</td>\n",
       "      <td>$31.11 per hour, plus super</td>\n",
       "      <td>Randwick, Sydney NSW</td>\n",
       "      <td>Retail Assistants (Retail &amp; Consumer Products)</td>\n",
       "      <td>Casual/Vacation</td>\n",
       "      <td>Independent Living Specialists is a fast-growi...</td>\n",
       "      <td>Do you have customer service experience?Do you...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73899163?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>Studio Assistant</td>\n",
       "      <td>Cendre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oxenford, Gold Coast QLD</td>\n",
       "      <td>Pickers &amp; Packers (Manufacturing, Transport &amp; ...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>Cendré is a revered e-commerce jewellery brand...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73875587?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>Junior IT Support Officer</td>\n",
       "      <td>Hare &amp; Forbes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Northmead, Sydney NSW</td>\n",
       "      <td>Help Desk &amp; IT Support (Information &amp; Communic...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>Parramatta locationWork with a close-knit, exp...</td>\n",
       "      <td>Do you have demonstrated experience diagnosing...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73868216?type=stan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              job_title  \\\n",
       "2610                         Level 2/3 Support Engineer   \n",
       "2611  NIGHT SHIFT WAREHOUSE TEAM LEADER WANTED WETHE...   \n",
       "2612                            Casual Retail Assistant   \n",
       "2613                                   Studio Assistant   \n",
       "2614                          Junior IT Support Officer   \n",
       "\n",
       "                             company                       salary  \\\n",
       "2610         Fuse Technology Pty Ltd                          NaN   \n",
       "2611                     Labourforce     $47 per hour + penalties   \n",
       "2612  Independent Living Specialists  $31.11 per hour, plus super   \n",
       "2613                          Cendre                          NaN   \n",
       "2614                   Hare & Forbes                          NaN   \n",
       "\n",
       "                    job_location  \\\n",
       "2610                  Sydney NSW   \n",
       "2611  Wetherill Park, Sydney NSW   \n",
       "2612        Randwick, Sydney NSW   \n",
       "2613    Oxenford, Gold Coast QLD   \n",
       "2614       Northmead, Sydney NSW   \n",
       "\n",
       "                                                   post         job_type  \\\n",
       "2610  Help Desk & IT Support (Information & Communic...        Full time   \n",
       "2611  Warehousing, Storage & Distribution (Manufactu...    Contract/Temp   \n",
       "2612     Retail Assistants (Retail & Consumer Products)  Casual/Vacation   \n",
       "2613  Pickers & Packers (Manufacturing, Transport & ...        Full time   \n",
       "2614  Help Desk & IT Support (Information & Communic...        Full time   \n",
       "\n",
       "                                               job_desc  \\\n",
       "2610  The opportunityAs part of our exciting growth ...   \n",
       "2611  Our client is one of Australia's leading Manuf...   \n",
       "2612  Independent Living Specialists is a fast-growi...   \n",
       "2613  Cendré is a revered e-commerce jewellery brand...   \n",
       "2614  Parramatta locationWork with a close-knit, exp...   \n",
       "\n",
       "                                            company_qns job_posted_date  \\\n",
       "2610  Which of the following statements best describ...      2024-02-21   \n",
       "2611                                                NaN      2024-02-21   \n",
       "2612  Do you have customer service experience?Do you...      2024-02-21   \n",
       "2613                                                NaN      2024-02-21   \n",
       "2614  Do you have demonstrated experience diagnosing...      2024-02-21   \n",
       "\n",
       "                                               job_link  \n",
       "2610  https://www.seek.com.au/job/73930150?type=stan...  \n",
       "2611  https://www.seek.com.au/job/73870879?type=stan...  \n",
       "2612  https://www.seek.com.au/job/73899163?type=stan...  \n",
       "2613  https://www.seek.com.au/job/73875587?type=stan...  \n",
       "2614  https://www.seek.com.au/job/73868216?type=stan...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reading the web scrapped data from CSV file, setting the index column\n",
    "df = pd.read_csv(\"job_data.csv\", index_col=0)\n",
    "\n",
    "# displaying the raw data\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d79c30-ba28-4b8b-a6ee-425ac8107091",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 9800\n",
      "Number of Columns: 10\n",
      "Index(['job_title', 'company', 'salary', 'job_location', 'post', 'job_type',\n",
      "       'job_desc', 'company_qns', 'job_posted_date', 'job_link'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# display the number of rows, columns and the column names\n",
    "def display_shape_and_colnames(df):\n",
    "    print(\"Number of Rows:\", df.shape[0])\n",
    "    print(\"Number of Columns:\", df.shape[1])\n",
    "    print(df.columns)\n",
    "    \n",
    "display_shape_and_colnames(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6837f9d0-3dc3-4986-a9df-337ddc17e9b7",
   "metadata": {},
   "source": [
    "Some of our __column names__ are __redundant__ because we are working with job data.\n",
    "\n",
    "Let's delete the prefix __\"job\"__ from our column names.\n",
    "\n",
    "Some other __column names__ are __abbreviated__ (e.g. \"job_desc\", \"company_qns\").\n",
    "\n",
    "Let's __replace them with full names__ so we can have accurate column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8e2f7a-9d56-4e39-8b51-b145d797646c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>company_questions</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Experienced Support Worker (PPT &amp; CAS)</td>\n",
       "      <td>Ability Gateway</td>\n",
       "      <td>$35.50 per hour [PPT]</td>\n",
       "      <td>Wagga Wagga, Wagga Wagga &amp; Riverina NSW</td>\n",
       "      <td>Aged &amp; Disability Support (Community Services ...</td>\n",
       "      <td>Part time</td>\n",
       "      <td>About usWe are an outcome focused NDIS service...</td>\n",
       "      <td>Do you own or have regular access to a car?Whi...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73909631?type=prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regional Manager - Inspire@HOME</td>\n",
       "      <td>CatholicCare Tasmania</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Launceston, Launceston &amp; North East TAS</td>\n",
       "      <td>Child Welfare, Youth &amp; Family Services (Commun...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>CatholicCare Tasmania is the primary social se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73909232?type=prom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title                company  \\\n",
       "0  Experienced Support Worker (PPT & CAS)        Ability Gateway   \n",
       "1         Regional Manager - Inspire@HOME  CatholicCare Tasmania   \n",
       "\n",
       "                  salary                                 location  \\\n",
       "0  $35.50 per hour [PPT]  Wagga Wagga, Wagga Wagga & Riverina NSW   \n",
       "1                    NaN  Launceston, Launceston & North East TAS   \n",
       "\n",
       "                                          department       type  \\\n",
       "0  Aged & Disability Support (Community Services ...  Part time   \n",
       "1  Child Welfare, Youth & Family Services (Commun...  Full time   \n",
       "\n",
       "                                         description  \\\n",
       "0  About usWe are an outcome focused NDIS service...   \n",
       "1  CatholicCare Tasmania is the primary social se...   \n",
       "\n",
       "                                   company_questions posted_date  \\\n",
       "0  Do you own or have regular access to a car?Whi...  2024-02-21   \n",
       "1                                                NaN  2024-02-21   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.seek.com.au/job/73909631?type=prom...  \n",
       "1  https://www.seek.com.au/job/73909232?type=prom...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_colnames(df):\n",
    "    # delete the prefix \"job_\" on our column names\n",
    "    for column_name in df.columns.to_list():\n",
    "        if column_name.startswith(\"job_\"):\n",
    "            df.rename(columns={column_name : column_name.lstrip(\"job_\")}, inplace=True)\n",
    "\n",
    "    # rename abbreviated column names\n",
    "    df.rename(columns={'desc':'description', 'company_qns':'company_questions', 'post':'department'}, inplace=True)\n",
    "\n",
    "clean_colnames(df)\n",
    "# display clean column names\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf7c020-0c61-4025-8b53-9283211d7ff5",
   "metadata": {},
   "source": [
    "Now let's undestand all of our columns by providing a description to each one:\n",
    "- __title__: title of the posted job\n",
    "- __company__: name of the company that has posted the job\n",
    "- __salary__: salary range for the job, can be defined per hour, monthly, annually, etc.\n",
    "- __location__: geographical location of the job or company\n",
    "- __department__: field or department of the job (e.g. IT, Sales, etc.)\n",
    "- __description__: long description of the job posting\n",
    "- __company_questions__: questions issued by the company to the applicants, according to the post\n",
    "- __posted_date__: format yyyy-mm-dd\n",
    "- __link__: link of the job posting\n",
    "\n",
    "Now that we have a general understanding of our web scrapped data. \n",
    "\n",
    "Let's go ahead to the next step to perform our data wrangling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb7c8b0-5e4d-44bc-9c1f-253acadabe00",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2. Data Wrangling (cleaning, formatting, structuring, validating)\n",
    "\n",
    "This is one crucial step as we are dealing with real-world data that is often unclean, and needs lost of processing.\n",
    "\n",
    "But, before performing any action, let's learn our data by doing some basic analysis.\n",
    "\n",
    "We will check the following stats by implementing functions:\n",
    "- missing values per column\n",
    "- duplicated rows\n",
    "- number of unique values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22d8490f-0b52-4dea-ac76-d5fd4e4b5dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Missing Values\n",
      "title                   0\n",
      "company                 0\n",
      "salary               5216\n",
      "location                0\n",
      "department              0\n",
      "type                    0\n",
      "description             0\n",
      "company_questions    5034\n",
      "posted_date             0\n",
      "link                    0\n",
      "dtype: int64\n",
      "\n",
      "% Missing Values\n",
      "title                 0.000000\n",
      "company               0.000000\n",
      "salary               53.224490\n",
      "location              0.000000\n",
      "department            0.000000\n",
      "type                  0.000000\n",
      "description           0.000000\n",
      "company_questions    51.367347\n",
      "posted_date           0.000000\n",
      "link                  0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def check_missing_values(df):\n",
    "    # check for number of missing values per column\n",
    "    print(\"# Missing Values\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    # check for % of missing values\n",
    "    print(\"\\n% Missing Values\")\n",
    "    print(df.isna().mean() * 100)\n",
    "    \n",
    "check_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9511b597-6839-49ed-9fcb-852fce895035",
   "metadata": {},
   "source": [
    "As expected, many job posts do not include a salary range or any information about the salary.\n",
    "\n",
    "It is no surprise that __more than half of our data has missing values for salary__.\n",
    "\n",
    "On the other hand, we also have __more than half missing values for the company questions column__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c435ba31-552a-4e09-898d-80df73894ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Duplicated Values\n",
      "944\n",
      "\n",
      "% Duplicated Values\n",
      "9.63265306122449\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>company_questions</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2587</th>\n",
       "      <td>Pick Packers</td>\n",
       "      <td>Action Workforce</td>\n",
       "      <td>35</td>\n",
       "      <td>Maddington, Perth WA</td>\n",
       "      <td>Warehousing, Storage &amp; Distribution (Manufactu...</td>\n",
       "      <td>Casual/Vacation</td>\n",
       "      <td>Action Workforce are looking for Experienced P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73901168?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>Accounts Person- KALGOORLIE RESIDENTS ONLY</td>\n",
       "      <td>Golden mile cleaning services</td>\n",
       "      <td>$30 – $33.50 per hour</td>\n",
       "      <td>Kalgoorlie, Kalgoorlie, Goldfields &amp; Esperance WA</td>\n",
       "      <td>Administrative Assistants (Administration &amp; Of...</td>\n",
       "      <td>Part time</td>\n",
       "      <td>Job Title: Accounts Person We are currently se...</td>\n",
       "      <td>Which of the following statements best describ...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73908087?type=prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2603</th>\n",
       "      <td>Warehouse Assistant</td>\n",
       "      <td>Omni Recruit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Truganina, Melbourne VIC</td>\n",
       "      <td>Pickers &amp; Packers (Manufacturing, Transport &amp; ...</td>\n",
       "      <td>Casual/Vacation</td>\n",
       "      <td>Business is booming and we are currently seeki...</td>\n",
       "      <td>Do you agree to the privacy policy of Omni Rec...</td>\n",
       "      <td>2024-02-20</td>\n",
       "      <td>https://www.seek.com.au/job/73863322?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>Casual Retail Assistant</td>\n",
       "      <td>Independent Living Specialists</td>\n",
       "      <td>$31.11 per hour, plus super</td>\n",
       "      <td>Randwick, Sydney NSW</td>\n",
       "      <td>Retail Assistants (Retail &amp; Consumer Products)</td>\n",
       "      <td>Casual/Vacation</td>\n",
       "      <td>Independent Living Specialists is a fast-growi...</td>\n",
       "      <td>Do you have customer service experience?Do you...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73899163?type=stan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title  \\\n",
       "2587                                Pick Packers   \n",
       "2593  Accounts Person- KALGOORLIE RESIDENTS ONLY   \n",
       "2603                         Warehouse Assistant   \n",
       "2612                     Casual Retail Assistant   \n",
       "\n",
       "                             company                       salary  \\\n",
       "2587                Action Workforce                           35   \n",
       "2593   Golden mile cleaning services        $30 – $33.50 per hour   \n",
       "2603                    Omni Recruit                          NaN   \n",
       "2612  Independent Living Specialists  $31.11 per hour, plus super   \n",
       "\n",
       "                                               location  \\\n",
       "2587                               Maddington, Perth WA   \n",
       "2593  Kalgoorlie, Kalgoorlie, Goldfields & Esperance WA   \n",
       "2603                           Truganina, Melbourne VIC   \n",
       "2612                               Randwick, Sydney NSW   \n",
       "\n",
       "                                             department             type  \\\n",
       "2587  Warehousing, Storage & Distribution (Manufactu...  Casual/Vacation   \n",
       "2593  Administrative Assistants (Administration & Of...        Part time   \n",
       "2603  Pickers & Packers (Manufacturing, Transport & ...  Casual/Vacation   \n",
       "2612     Retail Assistants (Retail & Consumer Products)  Casual/Vacation   \n",
       "\n",
       "                                            description  \\\n",
       "2587  Action Workforce are looking for Experienced P...   \n",
       "2593  Job Title: Accounts Person We are currently se...   \n",
       "2603  Business is booming and we are currently seeki...   \n",
       "2612  Independent Living Specialists is a fast-growi...   \n",
       "\n",
       "                                      company_questions posted_date  \\\n",
       "2587                                                NaN  2024-02-21   \n",
       "2593  Which of the following statements best describ...  2024-02-21   \n",
       "2603  Do you agree to the privacy policy of Omni Rec...  2024-02-20   \n",
       "2612  Do you have customer service experience?Do you...  2024-02-21   \n",
       "\n",
       "                                                   link  \n",
       "2587  https://www.seek.com.au/job/73901168?type=stan...  \n",
       "2593  https://www.seek.com.au/job/73908087?type=prom...  \n",
       "2603  https://www.seek.com.au/job/73863322?type=stan...  \n",
       "2612  https://www.seek.com.au/job/73899163?type=stan...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_duplicated_values(df):\n",
    "    # check for number of duplicated values\n",
    "    print(\"# Duplicated Values\")\n",
    "    print(df.duplicated().sum())\n",
    "    \n",
    "    # check for % of duplicated values\n",
    "    print(\"\\n% Duplicated Values\")\n",
    "    print(df.duplicated().mean() * 100)\n",
    "\n",
    "check_duplicated_values(df)\n",
    "\n",
    "df[df.duplicated()].tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b355ec06-cbde-42b9-8d5c-f3c060df6f41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Part time', 'Full time', 'Casual/Vacation', 'Contract/Temp',\n",
       "       'Contract/Temp, Casual/Vacation, Part time',\n",
       "       'Contract/Temp, Casual/Vacation, Full time, Part time',\n",
       "       'Contract/Temp, Part time', 'Casual/Vacation, Full time'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141d4cf-c61e-4847-bc3f-49c84247f2d6",
   "metadata": {},
   "source": [
    "Some considerable amount of our data __(around 9.5%) are duplicated__ rows.\n",
    "\n",
    "This can be __dangerous for analysis__, as it can affect multiple metrics and our model training.\n",
    "\n",
    "We have to __deal with these duplicated values__ in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f48a36-28e8-4ff6-8df1-57c0f9c822f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique Values per Column\n",
      "'title' # of unique values: 5655\n",
      "'company' # of unique values: 4965\n",
      "'salary' # of unique values: 2645\n",
      "'location' # of unique values: 1448\n",
      "'department' # of unique values: 451\n",
      "'type' # of unique values: 8\n",
      "'description' # of unique values: 7958\n",
      "'company_questions' # of unique values: 2730\n",
      "'posted_date' # of unique values: 95\n",
      "'link' # of unique values: 8664\n",
      "\n",
      "% Unique Values per Column\n",
      "'title' % of unique values: 57.7 %\n",
      "'company' % of unique values: 50.66 %\n",
      "'salary' % of unique values: 26.99 %\n",
      "'location' % of unique values: 14.78 %\n",
      "'department' % of unique values: 4.6 %\n",
      "'type' % of unique values: 0.08 %\n",
      "'description' % of unique values: 81.2 %\n",
      "'company_questions' % of unique values: 27.86 %\n",
      "'posted_date' % of unique values: 0.97 %\n",
      "'link' % of unique values: 88.41 %\n"
     ]
    }
   ],
   "source": [
    "def check_nunique_values(df):\n",
    "    # check number of unique values per column\n",
    "    print(\"# Unique Values per Column\")\n",
    "    for col in df.columns:\n",
    "        print(\"'\"+col+\"'\", \"# of unique values:\", df[col].nunique())\n",
    "        \n",
    "    # check % of unique values per column (relative to number of total rows in the dataset)\n",
    "    print(\"\\n% Unique Values per Column\")\n",
    "    for col in df.columns:\n",
    "        print(\"'\"+col+\"'\", \"% of unique values:\", round(df[col].nunique() * 100 / df.shape[0], 2), \"%\")\n",
    "        \n",
    "check_nunique_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e88d6-fc2a-4f32-aa18-43a753474b66",
   "metadata": {},
   "source": [
    "Some of our columns have a __large amount of unique values__.\n",
    "\n",
    "Although we still have not processed our values, we must __seek to reduce the number of unique values through data processing__.\n",
    "\n",
    "There are some columns with vast amount of unique values __(>50% of total rows)__. These columns are:\n",
    "- title\n",
    "- company\n",
    "- description\n",
    "- link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3423f09-458a-47af-a4e3-e1ddb1371a51",
   "metadata": {},
   "source": [
    "Let's start __dealing with the unique values per column.__\n",
    "\n",
    "To reduce the number of unique values, let's apply some NLP methods to each column values.\n",
    "\n",
    "We will start with some basic cleaning that includes:\n",
    "- removing punctuation\n",
    "- removing digits\n",
    "- lower case all letters\n",
    "- removing extra whitespaces\n",
    "\n",
    "To accomplish this, we will implement a __class called NLP__ that will __contain all__ of our implemented __NLP methods/techniques__ that will be __applied on our data__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9b61c0e-6e6e-44a3-be52-9ddcefbd6691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "# class containing our implemented NLP techniques and methods\n",
    "class NLP():\n",
    "    \n",
    "    # remove all punctuation from a word (string)\n",
    "    def remove_punctuation(self, word):\n",
    "        if not isinstance(word, str): return word\n",
    "        return word.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # remove all digits/numbers from a word (string)\n",
    "    def remove_digits(self, word):\n",
    "        if not isinstance(word, str): return word\n",
    "        return re.sub(r'\\d+', '', word)\n",
    "    \n",
    "    # checks if word is a string and returns lower cased\n",
    "    def lower_word(self, word):\n",
    "        if not isinstance(word, str): return word\n",
    "        return word.lower()\n",
    "\n",
    "    # perform basic operations to clean 1 column of a dataframe\n",
    "    def basic_clean_text_column(self, df, colname):\n",
    "        print(\"Basic cleaning on column '\" + colname + \"':\")\n",
    "        nunique = df[colname].nunique()\n",
    "        print(\"# Unique values before cleaning:\", df[colname].nunique())\n",
    "        for value in df[colname].unique():\n",
    "            # save original value to replace later\n",
    "            og_value = value\n",
    "            \n",
    "            # if we are dealing with a null value, don't modify anything\n",
    "            if value is np.nan: continue\n",
    "            \n",
    "            # remove punctuation from the column value\n",
    "            value = self.remove_punctuation(str(value))\n",
    "            \n",
    "            # remove digits from column value\n",
    "            value = self.remove_digits(value)\n",
    "            \n",
    "            # lower case column value\n",
    "            value = self.lower_word(value)\n",
    "            \n",
    "            # word tokenize the column value\n",
    "            word_tokens = word_tokenize(value)\n",
    "            \n",
    "            # update df value in place\n",
    "            df[colname].replace(og_value, ' '.join(word_tokens), inplace=True)\n",
    "        new_nunique = df[colname].nunique()\n",
    "        print(\"# Unique values after cleaning:\", df[colname].nunique())\n",
    "        print(\"% of unique values reduction:\", round(100 - (new_nunique*100/nunique),2), \"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d6c780-abb2-4a8d-b79a-3a64c2f89fdf",
   "metadata": {},
   "source": [
    "Now that we have implemented a class for our methods,\n",
    "\n",
    "let's go ahead and __apply a basic cleaning on all our columns__.\n",
    "\n",
    "Then, we can __compare values before vs after cleaning__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73deb158-a819-4850-8576-6ddbcc24a1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic cleaning on column 'title':\n",
      "# Unique values before cleaning: 5655\n",
      "# Unique values after cleaning: 5541\n",
      "% of unique values reduction: 2.02 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>clean title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Experienced Support Worker (PPT &amp; CAS)</td>\n",
       "      <td>experienced support worker ppt cas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regional Manager - Inspire@HOME</td>\n",
       "      <td>regional manager inspirehome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Family Support Worker</td>\n",
       "      <td>family support worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CPS Case Manager</td>\n",
       "      <td>cps case manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Intake Worker</td>\n",
       "      <td>intake worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>Level 2/3 Support Engineer</td>\n",
       "      <td>level support engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>NIGHT SHIFT WAREHOUSE TEAM LEADER WANTED WETHE...</td>\n",
       "      <td>night shift warehouse team leader wanted wethe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>Casual Retail Assistant</td>\n",
       "      <td>casual retail assistant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>Studio Assistant</td>\n",
       "      <td>studio assistant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>Junior IT Support Officer</td>\n",
       "      <td>junior it support officer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                Experienced Support Worker (PPT & CAS)   \n",
       "1                       Regional Manager - Inspire@HOME   \n",
       "2                                 Family Support Worker   \n",
       "3                                      CPS Case Manager   \n",
       "4                                         Intake Worker   \n",
       "...                                                 ...   \n",
       "2610                         Level 2/3 Support Engineer   \n",
       "2611  NIGHT SHIFT WAREHOUSE TEAM LEADER WANTED WETHE...   \n",
       "2612                            Casual Retail Assistant   \n",
       "2613                                   Studio Assistant   \n",
       "2614                          Junior IT Support Officer   \n",
       "\n",
       "                                            clean title  \n",
       "0                    experienced support worker ppt cas  \n",
       "1                          regional manager inspirehome  \n",
       "2                                 family support worker  \n",
       "3                                      cps case manager  \n",
       "4                                         intake worker  \n",
       "...                                                 ...  \n",
       "2610                             level support engineer  \n",
       "2611  night shift warehouse team leader wanted wethe...  \n",
       "2612                            casual retail assistant  \n",
       "2613                                   studio assistant  \n",
       "2614                          junior it support officer  \n",
       "\n",
       "[9800 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_and_compare_column(df, colname):\n",
    "    # save raw title data into a new dataframe just to compare before vs after cleaning\n",
    "    df_compare = df[[colname]].copy()\n",
    "\n",
    "    # perform the basic cleaning on the title column\n",
    "    nlp = NLP()\n",
    "    nlp.basic_clean_text_column(df, colname)\n",
    "\n",
    "    # compare before vs after\n",
    "    df_compare[\"clean \"+colname] = df[colname]\n",
    "    display(df_compare)\n",
    "\n",
    "clean_and_compare_column(df, 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56efc01-a23d-4b50-ae8a-bc3979da51b6",
   "metadata": {},
   "source": [
    "After this __1st experiment__ of __cleaning the 'title' column__ we notice that we have __reduced the number of unique values by 114__.\n",
    "\n",
    "Which is equivalent of aproximately __2% of the total unique values__, __not a significant reduction__.\n",
    "\n",
    "However, we have considerably clean our raw texts, and this will allow us to apply further NLP techniques that will have better results on reducing the number of unique values.\n",
    "\n",
    "But before getting ahead, let's __apply the same basic cleaning on the rest of our text columns__ such as:\n",
    "- title\n",
    "- company\n",
    "- location\n",
    "- department\n",
    "- description\n",
    "- company_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76605134-4666-4978-9972-8697ac556f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic cleaning on column 'company':\n",
      "# Unique values before cleaning: 4965\n",
      "# Unique values after cleaning: 4965\n",
      "% of unique values reduction: 0.0 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>clean company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ability Gateway</td>\n",
       "      <td>ability gateway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatholicCare Tasmania</td>\n",
       "      <td>catholiccare tasmania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Community Gro</td>\n",
       "      <td>community gro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Open Minds</td>\n",
       "      <td>open minds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Centre for Women &amp; Co.</td>\n",
       "      <td>the centre for women co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>Fuse Technology Pty Ltd</td>\n",
       "      <td>fuse technology pty ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>Labourforce</td>\n",
       "      <td>labourforce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>Independent Living Specialists</td>\n",
       "      <td>independent living specialists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>Cendre</td>\n",
       "      <td>cendre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>Hare &amp; Forbes</td>\n",
       "      <td>hare forbes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             company                   clean company\n",
       "0                    Ability Gateway                 ability gateway\n",
       "1              CatholicCare Tasmania           catholiccare tasmania\n",
       "2                      Community Gro                   community gro\n",
       "3                         Open Minds                      open minds\n",
       "4         The Centre for Women & Co.         the centre for women co\n",
       "...                              ...                             ...\n",
       "2610         Fuse Technology Pty Ltd         fuse technology pty ltd\n",
       "2611                     Labourforce                     labourforce\n",
       "2612  Independent Living Specialists  independent living specialists\n",
       "2613                          Cendre                          cendre\n",
       "2614                   Hare & Forbes                     hare forbes\n",
       "\n",
       "[9800 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic cleaning on column 'location':\n",
      "# Unique values before cleaning: 1448\n",
      "# Unique values after cleaning: 1448\n",
      "% of unique values reduction: 0.0 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>clean location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wagga Wagga, Wagga Wagga &amp; Riverina NSW</td>\n",
       "      <td>wagga wagga wagga wagga riverina nsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Launceston, Launceston &amp; North East TAS</td>\n",
       "      <td>launceston launceston north east tas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Townsville, Northern QLD</td>\n",
       "      <td>townsville northern qld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nambour, Sunshine Coast QLD</td>\n",
       "      <td>nambour sunshine coast qld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Underwood, Brisbane QLD</td>\n",
       "      <td>underwood brisbane qld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>sydney nsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>Wetherill Park, Sydney NSW</td>\n",
       "      <td>wetherill park sydney nsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>Randwick, Sydney NSW</td>\n",
       "      <td>randwick sydney nsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>Oxenford, Gold Coast QLD</td>\n",
       "      <td>oxenford gold coast qld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>Northmead, Sydney NSW</td>\n",
       "      <td>northmead sydney nsw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     location  \\\n",
       "0     Wagga Wagga, Wagga Wagga & Riverina NSW   \n",
       "1     Launceston, Launceston & North East TAS   \n",
       "2                    Townsville, Northern QLD   \n",
       "3                 Nambour, Sunshine Coast QLD   \n",
       "4                     Underwood, Brisbane QLD   \n",
       "...                                       ...   \n",
       "2610                               Sydney NSW   \n",
       "2611               Wetherill Park, Sydney NSW   \n",
       "2612                     Randwick, Sydney NSW   \n",
       "2613                 Oxenford, Gold Coast QLD   \n",
       "2614                    Northmead, Sydney NSW   \n",
       "\n",
       "                            clean location  \n",
       "0     wagga wagga wagga wagga riverina nsw  \n",
       "1     launceston launceston north east tas  \n",
       "2                  townsville northern qld  \n",
       "3               nambour sunshine coast qld  \n",
       "4                   underwood brisbane qld  \n",
       "...                                    ...  \n",
       "2610                            sydney nsw  \n",
       "2611             wetherill park sydney nsw  \n",
       "2612                   randwick sydney nsw  \n",
       "2613               oxenford gold coast qld  \n",
       "2614                  northmead sydney nsw  \n",
       "\n",
       "[9800 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic cleaning on column 'department':\n",
      "# Unique values before cleaning: 451\n",
      "# Unique values after cleaning: 451\n",
      "% of unique values reduction: 0.0 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department</th>\n",
       "      <th>clean department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aged &amp; Disability Support (Community Services ...</td>\n",
       "      <td>aged disability support community services dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Child Welfare, Youth &amp; Family Services (Commun...</td>\n",
       "      <td>child welfare youth family services community ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Child Welfare, Youth &amp; Family Services (Commun...</td>\n",
       "      <td>child welfare youth family services community ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Community Development (Community Services &amp; De...</td>\n",
       "      <td>community development community services devel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Child Welfare, Youth &amp; Family Services (Commun...</td>\n",
       "      <td>child welfare youth family services community ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>Help Desk &amp; IT Support (Information &amp; Communic...</td>\n",
       "      <td>help desk it support information communication...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>Warehousing, Storage &amp; Distribution (Manufactu...</td>\n",
       "      <td>warehousing storage distribution manufacturing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>Retail Assistants (Retail &amp; Consumer Products)</td>\n",
       "      <td>retail assistants retail consumer products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>Pickers &amp; Packers (Manufacturing, Transport &amp; ...</td>\n",
       "      <td>pickers packers manufacturing transport logistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>Help Desk &amp; IT Support (Information &amp; Communic...</td>\n",
       "      <td>help desk it support information communication...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             department  \\\n",
       "0     Aged & Disability Support (Community Services ...   \n",
       "1     Child Welfare, Youth & Family Services (Commun...   \n",
       "2     Child Welfare, Youth & Family Services (Commun...   \n",
       "3     Community Development (Community Services & De...   \n",
       "4     Child Welfare, Youth & Family Services (Commun...   \n",
       "...                                                 ...   \n",
       "2610  Help Desk & IT Support (Information & Communic...   \n",
       "2611  Warehousing, Storage & Distribution (Manufactu...   \n",
       "2612     Retail Assistants (Retail & Consumer Products)   \n",
       "2613  Pickers & Packers (Manufacturing, Transport & ...   \n",
       "2614  Help Desk & IT Support (Information & Communic...   \n",
       "\n",
       "                                       clean department  \n",
       "0     aged disability support community services dev...  \n",
       "1     child welfare youth family services community ...  \n",
       "2     child welfare youth family services community ...  \n",
       "3     community development community services devel...  \n",
       "4     child welfare youth family services community ...  \n",
       "...                                                 ...  \n",
       "2610  help desk it support information communication...  \n",
       "2611  warehousing storage distribution manufacturing...  \n",
       "2612         retail assistants retail consumer products  \n",
       "2613  pickers packers manufacturing transport logistics  \n",
       "2614  help desk it support information communication...  \n",
       "\n",
       "[9800 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic cleaning on column 'description':\n",
      "# Unique values before cleaning: 7958\n",
      "# Unique values after cleaning: 7928\n",
      "% of unique values reduction: 0.38 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>clean description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>About usWe are an outcome focused NDIS service...</td>\n",
       "      <td>about uswe are an outcome focused ndis service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatholicCare Tasmania is the primary social se...</td>\n",
       "      <td>catholiccare tasmania is the primary social se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Community Gro Inc is a community-based non-pro...</td>\n",
       "      <td>community gro inc is a communitybased nonprofi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As a Case Manager for Coastal Supports at Open...</td>\n",
       "      <td>as a case manager for coastal supports at open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>About Us and Our Team Culture   At The Centre ...</td>\n",
       "      <td>about us and our team culture at the centre fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>The opportunityAs part of our exciting growth ...</td>\n",
       "      <td>the opportunityas part of our exciting growth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>Our client is one of Australia's leading Manuf...</td>\n",
       "      <td>our client is one of australias leading manufa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>Independent Living Specialists is a fast-growi...</td>\n",
       "      <td>independent living specialists is a fastgrowin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>Cendré is a revered e-commerce jewellery brand...</td>\n",
       "      <td>cendré is a revered ecommerce jewellery brand ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>Parramatta locationWork with a close-knit, exp...</td>\n",
       "      <td>parramatta locationwork with a closeknit exper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            description  \\\n",
       "0     About usWe are an outcome focused NDIS service...   \n",
       "1     CatholicCare Tasmania is the primary social se...   \n",
       "2     Community Gro Inc is a community-based non-pro...   \n",
       "3     As a Case Manager for Coastal Supports at Open...   \n",
       "4     About Us and Our Team Culture   At The Centre ...   \n",
       "...                                                 ...   \n",
       "2610  The opportunityAs part of our exciting growth ...   \n",
       "2611  Our client is one of Australia's leading Manuf...   \n",
       "2612  Independent Living Specialists is a fast-growi...   \n",
       "2613  Cendré is a revered e-commerce jewellery brand...   \n",
       "2614  Parramatta locationWork with a close-knit, exp...   \n",
       "\n",
       "                                      clean description  \n",
       "0     about uswe are an outcome focused ndis service...  \n",
       "1     catholiccare tasmania is the primary social se...  \n",
       "2     community gro inc is a communitybased nonprofi...  \n",
       "3     as a case manager for coastal supports at open...  \n",
       "4     about us and our team culture at the centre fo...  \n",
       "...                                                 ...  \n",
       "2610  the opportunityas part of our exciting growth ...  \n",
       "2611  our client is one of australias leading manufa...  \n",
       "2612  independent living specialists is a fastgrowin...  \n",
       "2613  cendré is a revered ecommerce jewellery brand ...  \n",
       "2614  parramatta locationwork with a closeknit exper...  \n",
       "\n",
       "[9800 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic cleaning on column 'company_questions':\n",
      "# Unique values before cleaning: 2730\n",
      "# Unique values after cleaning: 2728\n",
      "% of unique values reduction: 0.07 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_questions</th>\n",
       "      <th>clean company_questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you own or have regular access to a car?Whi...</td>\n",
       "      <td>do you own or have regular access to a carwhic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which of the following statements best describ...</td>\n",
       "      <td>which of the following statements best describ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which of the following statements best describ...</td>\n",
       "      <td>which of the following statements best describ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>Which of the following statements best describ...</td>\n",
       "      <td>which of the following statements best describ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>Do you have customer service experience?Do you...</td>\n",
       "      <td>do you have customer service experiencedo you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>Do you have demonstrated experience diagnosing...</td>\n",
       "      <td>do you have demonstrated experience diagnosing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      company_questions  \\\n",
       "0     Do you own or have regular access to a car?Whi...   \n",
       "1                                                   NaN   \n",
       "2     Which of the following statements best describ...   \n",
       "3                                                   NaN   \n",
       "4     Which of the following statements best describ...   \n",
       "...                                                 ...   \n",
       "2610  Which of the following statements best describ...   \n",
       "2611                                                NaN   \n",
       "2612  Do you have customer service experience?Do you...   \n",
       "2613                                                NaN   \n",
       "2614  Do you have demonstrated experience diagnosing...   \n",
       "\n",
       "                                clean company_questions  \n",
       "0     do you own or have regular access to a carwhic...  \n",
       "1                                                   NaN  \n",
       "2     which of the following statements best describ...  \n",
       "3                                                   NaN  \n",
       "4     which of the following statements best describ...  \n",
       "...                                                 ...  \n",
       "2610  which of the following statements best describ...  \n",
       "2611                                                NaN  \n",
       "2612  do you have customer service experiencedo you ...  \n",
       "2613                                                NaN  \n",
       "2614  do you have demonstrated experience diagnosing...  \n",
       "\n",
       "[9800 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the remaining text columns that we need to perform a basic clean\n",
    "text_cols = ['company', 'location', 'department', 'description', 'company_questions']\n",
    "\n",
    "# implement a function to perform the cleaning on these columns\n",
    "def clean_and_compare_columns(df, cols):\n",
    "    for colname in cols:\n",
    "        clean_and_compare_column(df, colname)\n",
    "\n",
    "# call the implemented function\n",
    "clean_and_compare_columns(df, text_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a36db8-4d5f-45a3-8114-86353f62b8ee",
   "metadata": {},
   "source": [
    "It seems that __most of the columns have not reduced their number of unique values yet__.\n",
    "\n",
    "Let's take a look to the entire __dataframe__ in the __current clean version__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19515e7e-9668-4c0c-ab01-e785e61810ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>company_questions</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>experienced support worker ppt cas</td>\n",
       "      <td>ability gateway</td>\n",
       "      <td>$35.50 per hour [PPT]</td>\n",
       "      <td>wagga wagga wagga wagga riverina nsw</td>\n",
       "      <td>aged disability support community services dev...</td>\n",
       "      <td>Part time</td>\n",
       "      <td>about uswe are an outcome focused ndis service...</td>\n",
       "      <td>do you own or have regular access to a carwhic...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73909631?type=prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regional manager inspirehome</td>\n",
       "      <td>catholiccare tasmania</td>\n",
       "      <td>NaN</td>\n",
       "      <td>launceston launceston north east tas</td>\n",
       "      <td>child welfare youth family services community ...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>catholiccare tasmania is the primary social se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73909232?type=prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family support worker</td>\n",
       "      <td>community gro</td>\n",
       "      <td>$40 – $44 per hour</td>\n",
       "      <td>townsville northern qld</td>\n",
       "      <td>child welfare youth family services community ...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>community gro inc is a communitybased nonprofi...</td>\n",
       "      <td>which of the following statements best describ...</td>\n",
       "      <td>2024-02-19</td>\n",
       "      <td>https://www.seek.com.au/job/73832771?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cps case manager</td>\n",
       "      <td>open minds</td>\n",
       "      <td>$82k – 84k + super + salary packaging + benefits</td>\n",
       "      <td>nambour sunshine coast qld</td>\n",
       "      <td>community development community services devel...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>as a case manager for coastal supports at open...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73901240?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>intake worker</td>\n",
       "      <td>the centre for women co</td>\n",
       "      <td>$41 – $42 per hour</td>\n",
       "      <td>underwood brisbane qld</td>\n",
       "      <td>child welfare youth family services community ...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>about us and our team culture at the centre fo...</td>\n",
       "      <td>which of the following statements best describ...</td>\n",
       "      <td>2024-02-20</td>\n",
       "      <td>https://www.seek.com.au/job/73861002?type=stan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title                  company  \\\n",
       "0  experienced support worker ppt cas          ability gateway   \n",
       "1        regional manager inspirehome    catholiccare tasmania   \n",
       "2               family support worker            community gro   \n",
       "3                    cps case manager               open minds   \n",
       "4                       intake worker  the centre for women co   \n",
       "\n",
       "                                             salary  \\\n",
       "0                             $35.50 per hour [PPT]   \n",
       "1                                               NaN   \n",
       "2                                $40 – $44 per hour   \n",
       "3  $82k – 84k + super + salary packaging + benefits   \n",
       "4                                $41 – $42 per hour   \n",
       "\n",
       "                               location  \\\n",
       "0  wagga wagga wagga wagga riverina nsw   \n",
       "1  launceston launceston north east tas   \n",
       "2               townsville northern qld   \n",
       "3            nambour sunshine coast qld   \n",
       "4                underwood brisbane qld   \n",
       "\n",
       "                                          department       type  \\\n",
       "0  aged disability support community services dev...  Part time   \n",
       "1  child welfare youth family services community ...  Full time   \n",
       "2  child welfare youth family services community ...  Full time   \n",
       "3  community development community services devel...  Full time   \n",
       "4  child welfare youth family services community ...  Full time   \n",
       "\n",
       "                                         description  \\\n",
       "0  about uswe are an outcome focused ndis service...   \n",
       "1  catholiccare tasmania is the primary social se...   \n",
       "2  community gro inc is a communitybased nonprofi...   \n",
       "3  as a case manager for coastal supports at open...   \n",
       "4  about us and our team culture at the centre fo...   \n",
       "\n",
       "                                   company_questions posted_date  \\\n",
       "0  do you own or have regular access to a carwhic...  2024-02-21   \n",
       "1                                                NaN  2024-02-21   \n",
       "2  which of the following statements best describ...  2024-02-19   \n",
       "3                                                NaN  2024-02-21   \n",
       "4  which of the following statements best describ...  2024-02-20   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.seek.com.au/job/73909631?type=prom...  \n",
       "1  https://www.seek.com.au/job/73909232?type=prom...  \n",
       "2  https://www.seek.com.au/job/73832771?type=stan...  \n",
       "3  https://www.seek.com.au/job/73901240?type=stan...  \n",
       "4  https://www.seek.com.au/job/73861002?type=stan...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display our current dataframe version\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85414198-d252-42e7-aa85-323d9c671c52",
   "metadata": {},
   "source": [
    "One important step during text processing and cleaning is the __removal of stopwords__.\n",
    "\n",
    "We have seen __lots of stopwords accross our dataset__.\n",
    "\n",
    "Our next step for cleaning is to remove all those stopwords.\n",
    "\n",
    "However, there is a catch. We must __pay attention to certain words that have important meaning and are considered stopwords__.\n",
    "\n",
    "- __Example:__ The most common meaning of the word __\"it\"__ is considered a stopword. However, \"IT\" in job posting titles may refer to \"Information Technologies\".\n",
    "\n",
    "This example and many others need to be considered before just simply deleting all stopwords.\n",
    "\n",
    "On the other hand, __some words that are not considered stopwords may need to be deleted__. In those cases we need to add them as stopwords.\n",
    "\n",
    "To have a sense of which stopwords we must remove and keep, let's start by identifying 1, 2, and 3 length words from our columns.\n",
    "\n",
    "After taking a general look at them we may __identify which ones to remove, and which ones to keep__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c7fdc83-696a-4b3a-b235-05da92f397a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words of length 1 to 3 on column 'title'\n",
      "- Words Length 1\n",
      "['a', 'd', 'f', 'i', 'k', 'l', 'm', 'n', 'p', 's', 't', 'v', 'w', 'x', 'y', '–', '’', '💡', '🤝']\n",
      "- Words Length 2\n",
      "['ah', 'ai', 'am', 'an', 'ao', 'ap', 'ar', 'as', 'at', 'au', 'av', 'ba', 'bb', 'bi', 'bp', 'ca', 'cc', 'ci', 'co', 'cx', 'dc', 'do', 'ds', 'ea', 'el', 'er', 'fm', 'fq', 'ft', 'gc', 'gm', 'go', 'gp', 'hc', 'hm', 'hr', 'ic', 'in', 'it', 'iv', 'ld', 'le', 'lf', 'lo', 'ma', 'mc', 'md', 'mq', 'mr', 'ms', 'mt', 'my', 'nd', 'no', 'nt', 'od', 'of', 'on', 'oo', 'or', 'ot', 'pa', 'pc', 'ph', 'pm', 'po', 'pt', 'pw', 'px', 'qa', 'qc', 'rd', 're', 'rn', 'sa', 'sc', 'sr', 'st', 'sw', 'to', 'tq', 'up', 'us', 'vp', 'wa', 'we', 'yr', '⚽️']\n",
      "- Words Length 3\n",
      "['abn', 'acm', 'act', 'age', 'ags', 'aid', 'ain', 'air', 'ald', 'alh', 'ali', 'all', 'ame', 'and', 'anz', 'aod', 'app', 'aps', 'apy', 'arc', 'are', 'aso', 'asx', 'atm', 'aus', 'aws', 'bar', 'bas', 'bay', 'bdm', 'bft', 'bgs', 'bid', 'bms', 'bom', 'box', 'bus', 'bws', 'cad', 'car', 'cas', 'cbd', 'ccs', 'cdc', 'ceo', 'cfo', 'cmt', 'cmy', 'cnc', 'cns', 'coo', 'cpc', 'cps', 'crk', 'crm', 'csl', 'ctp', 'daf', 'day', 'dev', 'dfo', 'dfv', 'div', 'dna', 'dog', 'dry', 'due', 'eca', 'ecm', 'egm', 'eho', 'ehs', 'elc', 'elm', 'emu', 'end', 'eoi', 'esd', 'esg', 'eso', 'euc', 'exp', 'far', 'fit', 'fix', 'fld', 'foi', 'fom', 'for', 'fpa', 'ftc', 'fte', 'fun', 'gap', 'gas', 'gcf', 'get', 'gin', 'gis', 'gmp', 'gpc', 'gym', 'hbc', 'hcp', 'her', 'his', 'hsp', 'hub', 'icp', 'ict', 'iga', 'imc', 'ims', 'inc', 'ion', 'isa', 'iso', 'itc', 'its', 'itt', 'ivf', 'jay', 'jmf', 'job', 'key', 'kit', 'lab', 'law', 'llc', 'lms', 'ltd', 'lvl', 'mcv', 'mep', 'mgr', 'mid', 'moe', 'msp', 'mth', 'net', 'new', 'nfp', 'ngs', 'nmr', 'nnw', 'non', 'now', 'nsw', 'nth', 'num', 'occ', 'off', 'one', 'ops', 'org', 'otc', 'ote', 'our', 'out', 'pae', 'pay', 'pcp', 'per', 'pet', 'phd', 'pmo', 'png', 'pos', 'ppt', 'pqe', 'pre', 'psu', 'pts', 'pty', 'qld', 'qsr', 'rab', 'ras', 'ray', 'rda', 'rep', 'rpd', 'rto', 'sap', 'sea', 'seo', 'ses', 'set', 'sil', 'sme', 'smp', 'sor', 'spa', 'spt', 'stp', 'sub', 'syd', 'tas', 'tax', 'tcs', 'tea', 'the', 'tmp', 'top', 'trc', 'try', 'ttw', 'two', 'uni', 'ute', 'van', 'vce', 'veg', 'vic', 'vps', 'web', 'wet', 'wfa', 'wfh', 'whs', 'whv', 'wmc', 'woy', 'yha', 'you']\n"
     ]
    }
   ],
   "source": [
    "# return a list of lists, each list will contain the words of length 1, 2, 3... n\n",
    "def identify_words_len_1_to_n(df, colname, n):\n",
    "    # set n number of empty lists\n",
    "    words = [[] for _ in range(n)]\n",
    "    \n",
    "    # loop through unique values of the column\n",
    "    for value in df[colname].unique():\n",
    "        # if it's not a string, go to the next value\n",
    "        if not isinstance(value, str): continue\n",
    "        \n",
    "        # tokenize the value, loop through the words, if the word length its in range, add them to corresponding list\n",
    "        tokens = word_tokenize(value)\n",
    "        for word in tokens:\n",
    "            if len(word) <= n:\n",
    "                words[len(word)-1].append(word)\n",
    "                \n",
    "    # delete repeated values in the lists and sort them\n",
    "    words_len_1_to_n = [sorted(list(set(words_sublist))) for words_sublist in words]\n",
    "    \n",
    "    # print the results (each list)\n",
    "    print(\"Words of length 1 to\", n, \"on column '\"+colname+\"'\")\n",
    "    for i in range(n):\n",
    "        print(\"- Words Length\", i+1)\n",
    "        print(words_len_1_to_n[i])\n",
    "    return words_len_1_to_n\n",
    "\n",
    "words_len_1_to_3 = identify_words_len_1_to_n(df, 'title', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee8744-1d68-4a7c-8a6a-9a03916e76d3",
   "metadata": {},
   "source": [
    "For our column title all words length 1 need to be removed, as they don't bring any value to our analysis.\n",
    "\n",
    "The only 1-length string that will not be removed is the apostrophe to keep word consistency.\n",
    "- __’__ : apostrophe\n",
    "\n",
    "From our 2 length words, we will remove most of them except for the following common job accronyms:\n",
    "- __hr__ : Human Resources\n",
    "- __it__: Information Technology\n",
    "\n",
    "From the 3 length words, again we will remove most of them except for the following:\n",
    "- __ceo__: Chief Executive Officer\n",
    "- __cfo__: Chief Financial Officer\n",
    "- __aws__: Amazon Web Services\n",
    "- __pmo__: Project Management Office\n",
    "- __pcp__: Primary Care Physician\n",
    "- __crm__: Customer Relationship Management\n",
    "- __sap__: System Applications (ERP leader)\n",
    "- __app__: application\n",
    "- __dev__: developer\n",
    "- __lab__: laboratory\n",
    "- __web__: internet\n",
    "- __law__: self-explanatory\n",
    "\n",
    "Let's perform the same operation with the rest of the text columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "931825ec-bca3-4fa8-9704-a2034083630c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text columns: ['title', 'company', 'location', 'department', 'description', 'company_questions']\n",
      "Words max length: [3, 3, 3, 3, 2, 3]\n",
      "\n",
      "Words of length 1 to 3 on column 'title'\n",
      "- Words Length 1\n",
      "['a', 'd', 'f', 'i', 'k', 'l', 'm', 'n', 'p', 's', 't', 'v', 'w', 'x', 'y', '–', '’', '💡', '🤝']\n",
      "- Words Length 2\n",
      "['ah', 'ai', 'am', 'an', 'ao', 'ap', 'ar', 'as', 'at', 'au', 'av', 'ba', 'bb', 'bi', 'bp', 'ca', 'cc', 'ci', 'co', 'cx', 'dc', 'do', 'ds', 'ea', 'el', 'er', 'fm', 'fq', 'ft', 'gc', 'gm', 'go', 'gp', 'hc', 'hm', 'hr', 'ic', 'in', 'it', 'iv', 'ld', 'le', 'lf', 'lo', 'ma', 'mc', 'md', 'mq', 'mr', 'ms', 'mt', 'my', 'nd', 'no', 'nt', 'od', 'of', 'on', 'oo', 'or', 'ot', 'pa', 'pc', 'ph', 'pm', 'po', 'pt', 'pw', 'px', 'qa', 'qc', 'rd', 're', 'rn', 'sa', 'sc', 'sr', 'st', 'sw', 'to', 'tq', 'up', 'us', 'vp', 'wa', 'we', 'yr', '⚽️']\n",
      "- Words Length 3\n",
      "['abn', 'acm', 'act', 'age', 'ags', 'aid', 'ain', 'air', 'ald', 'alh', 'ali', 'all', 'ame', 'and', 'anz', 'aod', 'app', 'aps', 'apy', 'arc', 'are', 'aso', 'asx', 'atm', 'aus', 'aws', 'bar', 'bas', 'bay', 'bdm', 'bft', 'bgs', 'bid', 'bms', 'bom', 'box', 'bus', 'bws', 'cad', 'car', 'cas', 'cbd', 'ccs', 'cdc', 'ceo', 'cfo', 'cmt', 'cmy', 'cnc', 'cns', 'coo', 'cpc', 'cps', 'crk', 'crm', 'csl', 'ctp', 'daf', 'day', 'dev', 'dfo', 'dfv', 'div', 'dna', 'dog', 'dry', 'due', 'eca', 'ecm', 'egm', 'eho', 'ehs', 'elc', 'elm', 'emu', 'end', 'eoi', 'esd', 'esg', 'eso', 'euc', 'exp', 'far', 'fit', 'fix', 'fld', 'foi', 'fom', 'for', 'fpa', 'ftc', 'fte', 'fun', 'gap', 'gas', 'gcf', 'get', 'gin', 'gis', 'gmp', 'gpc', 'gym', 'hbc', 'hcp', 'her', 'his', 'hsp', 'hub', 'icp', 'ict', 'iga', 'imc', 'ims', 'inc', 'ion', 'isa', 'iso', 'itc', 'its', 'itt', 'ivf', 'jay', 'jmf', 'job', 'key', 'kit', 'lab', 'law', 'llc', 'lms', 'ltd', 'lvl', 'mcv', 'mep', 'mgr', 'mid', 'moe', 'msp', 'mth', 'net', 'new', 'nfp', 'ngs', 'nmr', 'nnw', 'non', 'now', 'nsw', 'nth', 'num', 'occ', 'off', 'one', 'ops', 'org', 'otc', 'ote', 'our', 'out', 'pae', 'pay', 'pcp', 'per', 'pet', 'phd', 'pmo', 'png', 'pos', 'ppt', 'pqe', 'pre', 'psu', 'pts', 'pty', 'qld', 'qsr', 'rab', 'ras', 'ray', 'rda', 'rep', 'rpd', 'rto', 'sap', 'sea', 'seo', 'ses', 'set', 'sil', 'sme', 'smp', 'sor', 'spa', 'spt', 'stp', 'sub', 'syd', 'tas', 'tax', 'tcs', 'tea', 'the', 'tmp', 'top', 'trc', 'try', 'ttw', 'two', 'uni', 'ute', 'van', 'vce', 'veg', 'vic', 'vps', 'web', 'wet', 'wfa', 'wfh', 'whs', 'whv', 'wmc', 'woy', 'yha', 'you']\n",
      "\n",
      "\n",
      "Words of length 1 to 3 on column 'company'\n",
      "- Words Length 1\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', '–', '’']\n",
      "- Words Length 2\n",
      "['ag', 'ah', 'ak', 'as', 'at', 'au', 'aw', 'ay', 'be', 'bp', 'br', 'bt', 'by', 'cg', 'ck', 'co', 'cs', 'da', 'de', 'do', 'dp', 'dr', 'ds', 'dt', 'dw', 'eg', 'em', 'ey', 'fc', 'fi', 'ft', 'ge', 'go', 'gp', 'hb', 'hd', 'hi', 'ho', 'hr', 'ic', 'in', 'ip', 'iq', 'it', 'jb', 'jd', 'jk', 'js', 'jv', 'ke', 'kh', 'ks', 'la', 'le', 'lf', 'lg', 'li', 'lj', 'lk', 'ly', 'mb', 'me', 'mj', 'mk', 'mq', 'mr', 'ms', 'mt', 'mv', 'mw', 'my', 'na', 'nc', 'nh', 'nl', 'no', 'nq', 'nt', 'nx', 'nz', 'of', 'on', 'op', 'oz', 'pc', 'pe', 'pl', 'pr', 'ps', 'pv', 'qb', 'qt', 'rd', 're', 'rk', 'rp', 'sa', 'sb', 'sc', 'se', 'sk', 'sm', 'st', 'ta', 'td', 'th', 'tj', 'tm', 'to', 'ts', 'ty', 'ua', 'up', 'uq', 'us', 'uu', 'va', 'wa', 'wb', 'wd', 'we', 'wh', 'ws', 'xm', 'xo', 'yd']\n",
      "- Words Length 3\n",
      "['abb', 'abs', 'abu', 'ace', 'ach', 'aco', 'act', 'adm', 'adp', 'adt', 'afl', 'agl', 'ahp', 'aia', 'aid', 'aig', 'aim', 'air', 'akd', 'alh', 'all', 'als', 'ame', 'amp', 'and', 'anu', 'anz', 'aon', 'apa', 'apg', 'apm', 'apn', 'aps', 'apt', 'aqa', 'arc', 'arg', 'arl', 'art', 'ary', 'asc', 'asi', 'asr', 'atb', 'atc', 'atf', 'aus', 'awx', 'axr', 'bai', 'bar', 'bay', 'bbm', 'bci', 'bdc', 'bee', 'bft', 'bge', 'bhp', 'big', 'bio', 'bis', 'bjs', 'bkb', 'bkh', 'bmd', 'boc', 'box', 'bpa', 'bpg', 'brc', 'car', 'cdm', 'cgc', 'chu', 'cim', 'cjd', 'cme', 'cns', 'cnw', 'com', 'cos', 'cox', 'cpb', 'cpc', 'cph', 'cpl', 'crl', 'csc', 'cse', 'csg', 'csi', 'csl', 'csr', 'ctp', 'cue', 'dan', 'day', 'ddp', 'dee', 'dfc', 'dfp', 'dgl', 'dgp', 'dhl', 'dla', 'dmc', 'dmk', 'dog', 'dpn', 'drd', 'dvp', 'dwc', 'dwf', 'dwp', 'dxc', 'ebl', 'ebm', 'eci', 'eco', 'edi', 'egm', 'egr', 'ehw', 'eis', 'elm', 'eml', 'emu', 'end', 'ent', 'era', 'est', 'eye', 'eym', 'ezi', 'ezy', 'fcb', 'fec', 'fip', 'fit', 'fja', 'fkg', 'fmp', 'fnw', 'for', 'fox', 'gap', 'gas', 'gem', 'gen', 'geo', 'get', 'gfg', 'gfr', 'gio', 'gmg', 'gmk', 'god', 'gow', 'gpa', 'gpc', 'gpt', 'gro', 'gsa', 'gse', 'gsl', 'gte', 'gur', 'gwg', 'gym', 'hba', 'hbf', 'hbt', 'hcf', 'hcl', 'hdr', 'heh', 'hfh', 'hhg', 'hic', 'hot', 'hpr', 'hrm', 'hro', 'hsi', 'hub', 'hvh', 'iag', 'ice', 'ict', 'iga', 'ilp', 'inc', 'ind', 'ing', 'iod', 'ior', 'ipm', 'ips', 'irt', 'iss', 'ive', 'ivf', 'iwc', 'jay', 'jbs', 'jcu', 'jde', 'jet', 'jfk', 'jhk', 'jjk', 'jjs', 'jkb', 'jll', 'jns', 'job', 'joe', 'jps', 'jqz', 'jsb', 'jtc', 'kas', 'kbh', 'kbr', 'key', 'kia', 'kid', 'klc', 'kod', 'kpi', 'ksh', 'kwb', 'lad', 'law', 'lci', 'ldk', 'lee', 'leo', 'lis', 'ljr', 'ljw', 'lka', 'lnw', 'lod', 'lsh', 'ltd', 'lws', 'mac', 'mak', 'man', 'map', 'max', 'may', 'mbh', 'mbs', 'mda', 'mdf', 'met', 'mfb', 'mfl', 'mga', 'mha', 'mid', 'mip', 'mjd', 'mla', 'mlc', 'mlg', 'mmh', 'mna', 'mnd', 'msj', 'msr', 'mss', 'mta', 'mtc', 'mtg', 'mti', 'nab', 'ncl', 'ndf', 'neo', 'nes', 'new', 'nex', 'ngo', 'ngu', 'nhs', 'nib', 'nkt', 'nls', 'not', 'now', 'nrg', 'nsl', 'nsp', 'nsw', 'nti', 'ntt', 'oak', 'ocs', 'ohc', 'oil', 'old', 'olg', 'omb', 'omc', 'one', 'opf', 'our', 'out', 'pac', 'pax', 'pbg', 'pcs', 'pet', 'pfd', 'pgf', 'ppc', 'ppg', 'prd', 'pro', 'psc', 'ptc', 'pte', 'ptg', 'pty', 'pvh', 'pvt', 'pwr', 'qbd', 'qbe', 'qgs', 'qib', 'qip', 'qld', 'qml', 'raa', 'ray', 'rbc', 'rbr', 'rcr', 'rda', 'rea', 'rec', 'red', 'reo', 'res', 'rfi', 'rhi', 'rio', 'rls', 'rmb', 'rms', 'roc', 'ron', 'roy', 'rpg', 'rpr', 'rsl', 'rsm', 'run', 'rws', 'sae', 'sam', 'sea', 'see', 'ses', 'sgs', 'sig', 'sil', 'six', 'sky', 'sms', 'snm', 'son', 'spa', 'srd', 'srg', 'sri', 'srt', 'ssi', 'sss', 'std', 'sun', 'swd', 'syc', 'syd', 'tac', 'tal', 'tas', 'tbh', 'tcr', 'tea', 'teg', 'ten', 'the', 'tip', 'tlb', 'tlh', 'tme', 'tna', 'tns', 'tom', 'top', 'trh', 'trs', 'tsa', 'ttp', 'twh', 'two', 'twr', 'ubx', 'ucx', 'ufs', 'ugl', 'ume', 'umq', 'uow', 'ups', 'utz', 'uwa', 'vaf', 'vet', 'vic', 'wal', 'way', 'wca', 'web', 'win', 'wlp', 'wmw', 'wow', 'wpp', 'wsd', 'wsp', 'xli', 'yap', 'yes', 'yfs', 'yha', 'you', 'ypm']\n",
      "\n",
      "\n",
      "Words of length 1 to 3 on column 'location'\n",
      "- Words Length 1\n",
      "['m']\n",
      "- Words Length 2\n",
      "['mt', 'nt', 'of', 'sa', 'st', 'wa']\n",
      "- Words Length 3\n",
      "['act', 'ayr', 'bar', 'bay', 'bli', 'box', 'dee', 'dry', 'emu', 'end', 'far', 'gap', 'hay', 'hom', 'isa', 'kew', 'koo', 'mid', 'moe', 'new', 'nsw', 'oak', 'old', 'qld', 'red', 'rup', 'tas', 'the', 'tom', 'two', 'vic', 'wee', 'why', 'woy']\n",
      "\n",
      "\n",
      "Words of length 1 to 3 on column 'department'\n",
      "- Words Length 1\n",
      "[]\n",
      "- Words Length 2\n",
      "['ae', 'ea', 'hr', 'it', 'md', 'ot', 'pa']\n",
      "- Words Length 3\n",
      "['ceo', 'coo', 'crm', 'icu', 'law', 'new', 'pre', 'tax', 'web']\n",
      "\n",
      "\n",
      "Words of length 1 to 2 on column 'description'\n",
      "- Words Length 1\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '©', '\\xad', '®', '°', '·', '»', '½', 'ø', '\\u200b', '–', '—', '‘', '’', '“', '”', '•', '…', '‼', '⁄', '€', '→', '⏳', '▸', '●', '☒', '✅', '✔', '✨', '⭐', '\\uf0d8', '�', '𝗮', '🌈', '🌍', '🌐', '🌞', '🌟', '🌱', '🌳', '🌴', '🌻', '🎁', '🎉', '🎥', '🎨', '🎯', '🏆', '🏊', '🏐', '🐦', '👉', '👍', '👪', '💡', '💪', '💰', '💲', '💵', '💻', '💼', '📃', '📈', '📊', '📍', '📝', '📞', '📢', '📧', '📱', '📸', '🔍', '🔝', '🔴', '😄', '😉', '😊', '😍', '🚀', '🚗', '🟡', '🤔', '🤝', '🤞', '🥰', '🩺']\n",
      "- Words Length 2\n",
      "['aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'ak', 'al', 'am', 'an', 'ao', 'ap', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'az', 'a•', 'bb', 'bc', 'bd', 'be', 'bf', 'bg', 'bi', 'bk', 'bl', 'bm', 'bn', 'bp', 'bs', 'bt', 'bu', 'bv', 'bw', 'bx', 'by', 'ca', 'cc', 'ce', 'cf', 'ch', 'ci', 'ck', 'cl', 'cm', 'cn', 'co', 'cq', 'cs', 'ct', 'cv', 'cw', 'cx', 'cy', 'da', 'db', 'dc', 'dd', 'de', 'df', 'dg', 'di', 'dj', 'dk', 'dm', 'dn', 'do', 'dp', 'dr', 'ds', 'dt', 'dv', 'dw', 'ea', 'eb', 'ec', 'ed', 'ee', 'ef', 'eg', 'ei', 'el', 'em', 'en', 'eo', 'ep', 'eq', 'er', 'es', 'et', 'eu', 'ev', 'ex', 'ey', 'fa', 'fb', 'fc', 'fe', 'ff', 'fg', 'fi', 'fm', 'fo', 'fp', 'fq', 'fs', 'ft', 'fx', 'fy', 'ga', 'gb', 'gc', 'gd', 'ge', 'gf', 'gi', 'gk', 'gl', 'gm', 'go', 'gp', 'gq', 'gs', 'gt', 'gu', 'gv', 'gw', 'gx', 'ha', 'hb', 'hc', 'hd', 'he', 'hg', 'hh', 'hi', 'hk', 'hl', 'ho', 'hp', 'hq', 'hr', 'hs', 'ht', 'hv', 'hw', 'ia', 'ib', 'ic', 'id', 'ie', 'if', 'ig', 'ii', 'im', 'in', 'io', 'ip', 'iq', 'ir', 'is', 'it', 'iv', 'iw', 'iy', 'jb', 'jd', 'jg', 'jh', 'jj', 'jk', 'jn', 'jo', 'jp', 'jr', 'js', 'jv', 'kb', 'kc', 'kd', 'ke', 'kf', 'kg', 'kh', 'kj', 'kk', 'km', 'kp', 'kt', 'kw', 'la', 'lb', 'lc', 'ld', 'le', 'lf', 'lg', 'li', 'lj', 'lk', 'll', 'ln', 'lo', 'lp', 'lr', 'lv', 'ly', 'ma', 'mb', 'mc', 'md', 'me', 'mf', 'mg', 'mj', 'mk', 'ml', 'mm', 'mo', 'mp', 'mq', 'mr', 'ms', 'mt', 'mv', 'mw', 'my', 'na', 'nb', 'nc', 'nd', 'ne', 'ng', 'nh', 'ni', 'no', 'np', 'nq', 'nt', 'nv', 'nw', 'nx', 'ny', 'nz', 'oc', 'od', 'oe', 'of', 'og', 'oh', 'ok', 'om', 'on', 'op', 'oq', 'or', 'os', 'ot', 'oz', 'pa', 'pc', 'pd', 'pe', 'pf', 'pg', 'ph', 'pi', 'pl', 'pm', 'pn', 'po', 'pp', 'pq', 'pr', 'ps', 'pt', 'pv', 'pw', 'px', 'qa', 'qb', 'qc', 'qe', 'qh', 'qm', 'qr', 'qs', 'qv', 'ra', 'rc', 'rd', 're', 'rf', 'rg', 'rh', 'ri', 'rn', 'ro', 'rp', 'rs', 'rv', 'rw', 'sa', 'sc', 'sd', 'se', 'sf', 'sg', 'si', 'sm', 'sn', 'so', 'sp', 'sq', 'sr', 'ss', 'st', 'sw', 's®', 's•', 'ta', 'tb', 'tc', 'td', 'te', 'th', 'ti', 'tj', 'tm', 'to', 'tq', 'ts', 'tt', 'tv', 'ty', 'tz', 'ua', 'uc', 'ud', 'uf', 'ui', 'uk', 'ul', 'um', 'un', 'up', 'uq', 'us', 'uu', 'uv', 'uw', 'ux', 'va', 'vc', 've', 'vm', 'vp', 'vr', 'vs', 'vu', 'vv', 'vw', 'vx', 'wa', 'wc', 'wd', 'we', 'wh', 'wj', 'wk', 'wl', 'wo', 'wr', 'ws', 'ww', 'wx', 'xg', 'xi', 'xm', 'xo', 'xp', 'xu', 'xx', 'ya', 'yd', 'yo', 'yr', 'ys', 'yu', 'za', 'zd', 'zp', 'zs', '£k', '\\xad–', '°c', 'ºc', '˚c', '\\u200ba', '–a', '•a', '…a', '\\u2063\\u2063', '◼️', '⚙️', '⚫️', '✅a', '✔️', '\\ufeffa', '\\ufeff\\ufeff', '𝗕𝗲', '𝗪𝗲', '𝘁𝗼', '𝘄𝗲', '🌟🔒', '🍌🍌', '👁️', '💸✅', '📲✨', '🔥🔥', '🙌🏼', '🚀✨', '🚀📍', '🤝🏼', '🤩🙌']\n",
      "\n",
      "\n",
      "Words of length 1 to 3 on column 'company_questions'\n",
      "- Words Length 1\n",
      "['a', 'c', 'i', 'p', 's', 'x', '–', '’']\n",
      "- Words Length 2\n",
      "['ad', 'am', 'an', 'as', 'at', 'ax', 'bb', 'be', 'by', 'ca', 'cg', 'ct', 'da', 'do', 'eg', 'hc', 'he', 'hr', 'ie', 'if', 'ii', 'in', 'is', 'it', 'iv', 'kg', 'me', 'mr', 'my', 'no', 'nv', 'nz', 'of', 'on', 'or', 'oz', 'pc', 'pm', 'qa', 'qc', 'rf', 'rg', 'so', 'th', 'to', 'up', 'us', 'wa', 'wd', 'we']\n",
      "- Words Length 3\n",
      "['aat', 'act', 'ads', 'aid', 'all', 'and', 'any', 'are', 'ask', 'bar', 'bas', 'bio', 'but', 'cad', 'can', 'car', 'cbd', 'cms', 'cpa', 'crm', 'css', 'day', 'did', 'dns', 'dot', 'end', 'etc', 'ezi', 'far', 'few', 'fit', 'for', 'gas', 'get', 'gmp', 'got', 'gym', 'had', 'has', 'hoc', 'how', 'hrs', 'hub', 'ict', 'iii', 'iso', 'its', 'ivf', 'job', 'key', 'law', 'lcs', 'led', 'let', 'max', 'may', 'mid', 'mlg', 'msp', 'naa', 'ner', 'new', 'not', 'now', 'npc', 'ntt', 'off', 'oil', 'one', 'ote', 'our', 'out', 'own', 'pay', 'per', 'pet', 'pmi', 'pms', 'pos', 'ppe', 'pre', 'ptc', 'put', 'qld', 'rcg', 'red', 'rhl', 'rsa', 'run', 'sap', 'say', 'see', 'sem', 'seo', 'set', 'shy', 'sub', 'tae', 'tax', 'tga', 'the', 'tot', 'tpb', 'tss', 'two', 'use', 'vic', 'vis', 'was', 'way', 'web', 'wed', 'wfh', 'who', 'why', 'wwc', 'yes', 'you']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def identify_words_len_1_to_n_columns(df, text_columns, ns):\n",
    "    # loop through the specified columns and identify the words of length 1 to n\n",
    "    words_per_col = []\n",
    "    for i, colname in enumerate(text_columns):\n",
    "        words_per_col.append(identify_words_len_1_to_n(df, colname, ns[i]))\n",
    "        print(\"\\n\")\n",
    "    return words_per_col\n",
    "\n",
    "# define the word lengths per text column\n",
    "text_cols = ['title', 'company', 'location', 'department', 'description', 'company_questions']\n",
    "word_max_lens = [3, 3, 3, 3, 2, 3]\n",
    "print(\"Text columns:\", text_cols, end='\\n')\n",
    "print(\"Words max length:\", word_max_lens, end='\\n\\n')\n",
    "words_per_col = identify_words_len_1_to_n_columns(df, text_cols, word_max_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca5431-4393-4836-9162-022294df3078",
   "metadata": {},
   "source": [
    "Now that we have identified more words to remove from our own data,\n",
    "\n",
    "let's __implement a function that removes all stopwords__ from the english language on top of the ones identified on our 1 to 3-length words analysis.\n",
    "\n",
    "Let's also keep in mind the __list of values that should not be removed__ (from the same analysis).\n",
    "\n",
    "But first, let's __define our additional stopwords and our exceptions__ as mentioned before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42d19419-9f71-491b-94ad-a9b50f0599ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import flatten # convert nested list into 1D list\n",
    "\n",
    "def set_additional_stopwords(words_per_col):\n",
    "    # set our additional stopwords making use of the identified 1 to 3 length words for each column\n",
    "    additionals = []\n",
    "    for column_words in words_per_col:\n",
    "        # make sure we only have unique values by using set\n",
    "        additionals.append(list(set(flatten(column_words))))\n",
    "    return additionals\n",
    "\n",
    "additionals = set_additional_stopwords(words_per_col)\n",
    "\n",
    "# set the exceptions manually based on our previous word length analysis\n",
    "exceptions = ['’', 'hr', 'it', 'ceo', 'cfo', 'aws', 'pmo', 'pcp', 'crm', 'sap', 'app', 'dev', 'lab', 'web', 'law']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9927e49-5f19-4935-a839-952683b4c775",
   "metadata": {},
   "source": [
    "Now that we have defined the additional stopwords and the exceptions,\n",
    "\n",
    "Let's actually __implement a new class that stores our stopwords removal methods__.\n",
    "\n",
    "We will use this class to perform our stop words removal __taking into account our additional stopwords and exceptions__.\n",
    "\n",
    "This will be __performed for every text column__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f8d54a3-735f-44bf-ade3-76d45eacc377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stopwords on column 'title'\n",
      "# Unique values with stopwords: 5541\n",
      "# Unique values without stopwords: 5398\n",
      "% of unique values reduction: 2.58 %\n",
      "\n",
      "Removing stopwords on column 'company'\n",
      "# Unique values with stopwords: 4965\n",
      "# Unique values without stopwords: 4704\n",
      "% of unique values reduction: 5.26 %\n",
      "\n",
      "Removing stopwords on column 'location'\n",
      "# Unique values with stopwords: 1448\n",
      "# Unique values without stopwords: 1439\n",
      "% of unique values reduction: 0.62 %\n",
      "\n",
      "Removing stopwords on column 'department'\n",
      "# Unique values with stopwords: 451\n",
      "# Unique values without stopwords: 449\n",
      "% of unique values reduction: 0.44 %\n",
      "\n",
      "Removing stopwords on column 'description'\n",
      "# Unique values with stopwords: 7928\n",
      "# Unique values without stopwords: 7927\n",
      "% of unique values reduction: 0.01 %\n",
      "\n",
      "Removing stopwords on column 'company_questions'\n",
      "# Unique values with stopwords: 2728\n",
      "# Unique values without stopwords: 2728\n",
      "% of unique values reduction: 0.0 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NLP_stopwords():\n",
    "    \n",
    "    def remove_stopwords_columns(self, df, colnames, additionals=[], exceptions=[]):\n",
    "        if additionals == []:\n",
    "            additionals = [[] for _ in range(len(colnameS))]\n",
    "        if len(colnames) != len(additionals):\n",
    "            raise Exception(\"Column names length must be equal to the additional stop words.\")\n",
    "\n",
    "        # remove stopwords on specified columns\n",
    "        for i, colname in enumerate(colnames):\n",
    "            self.remove_stopwords_column(df, colname, additionals[i], exceptions)\n",
    "\n",
    "    def remove_stopwords_column(self, df, colname, additional=[], exceptions=[]):\n",
    "        print(\"Removing stopwords on column '\" + colname + \"'\")\n",
    "        nunique = df[colname].nunique()\n",
    "        print(\"# Unique values with stopwords:\", df[colname].nunique())        \n",
    "        \n",
    "        # loop through unique values of the column\n",
    "        for value in df[colname].unique():\n",
    "            # make sure the value is a string\n",
    "            if not isinstance(value, str): continue\n",
    "            \n",
    "            # tokenize the unique column value\n",
    "            tokens = word_tokenize(value)\n",
    "\n",
    "            # remove stopwords\n",
    "            self.remove_stopwords(tokens, additional, exceptions)\n",
    "\n",
    "            # update df value in place\n",
    "            df[colname].replace(value, ' '.join(tokens), inplace=True)\n",
    "        \n",
    "        new_nunique = df[colname].nunique()\n",
    "        print(\"# Unique values without stopwords:\", df[colname].nunique())\n",
    "        print(\"% of unique values reduction:\", round(100 - (new_nunique*100/nunique),2), \"%\", end=\"\\n\\n\")\n",
    "\n",
    "    def remove_stopwords(self, tokens, additional=[], exceptions=[]):\n",
    "        # remove stopwords on a list of word tokens\n",
    "        i = 0\n",
    "        # add the additional parameter stopwords\n",
    "        total_stopwords = stopwords.words('english') + additional\n",
    "        while i < len(tokens):\n",
    "            word = tokens[i]\n",
    "            # if the word is in exceptions, don't remove it\n",
    "            if word in total_stopwords and word not in exceptions:\n",
    "                tokens.pop(i)\n",
    "                i -= 1\n",
    "            i += 1\n",
    "\n",
    "nlp = NLP_stopwords()\n",
    "nlp.remove_stopwords_columns(df, text_cols, additionals, exceptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3d1134-6300-4bd4-abf8-cac16ab542af",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that we have __removed stopwords__ (including additional analyzed extra stopwords and keeping some exceptions),\n",
    "\n",
    "we see that __unique values have been reduced on some low percentage__.\n",
    "\n",
    "However, our text columns __data now contain more clean data, with mostly only relevant words__.\n",
    "\n",
    "Let's perform one more step to __group our unique values__ and hopefully __reduce the number of uniques significantly__.\n",
    "\n",
    "For starters, let's __find our unigrams, bigrams, and trigrams__.\n",
    "\n",
    "Once again, we will __define a third NLP class to store our new implemented methods__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "984b295e-a235-46ae-91c8-f6aa4c4fcd08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NLP_ngrams():\n",
    "        \n",
    "    def get_column_ngram(self, df, colname, n=1):\n",
    "        # get an n-gram dictionary (example: bigrams) of 1 specified column\n",
    "        ngrams = {}\n",
    "        \n",
    "        # iterate through the rows of the column\n",
    "        for i, row in df[[colname]].iterrows():\n",
    "            # tokenize the value if it's a string\n",
    "            if not isinstance(row[0], str): continue\n",
    "            tokens = word_tokenize(row[0])\n",
    "            \n",
    "            # for each word in the word tokenization, add or update the dictionary of ngrams\n",
    "            for i in range(len(tokens) - n + 1):\n",
    "                ngram = ' '.join(tokens[i:i+n])\n",
    "                ngrams[ngram] = ngrams.get(ngram, 0) + 1\n",
    "        \n",
    "        # sort the dictionary of ngrams by value in descending order\n",
    "        return dict(sorted(ngrams.items(), key = lambda x : x[1], reverse=True))\n",
    "    \n",
    "    def get_column_ngrams(self, df, colname, n=1):\n",
    "        # get all n-grams from one specific column\n",
    "        # example: with n = 3, ngrams will store a list of 3 dictionaries: unigrams, bigrams, and trigrams\n",
    "        \n",
    "        if n < 1: raise Exception(\"n in n-grams must be an integer greater or equal to 1\")\n",
    "        ngrams = [{} for _ in range(n)]\n",
    "        # loop through n, populating the n-gram (unigram, bigrams, etc.)\n",
    "        for i in range(n):\n",
    "            ngrams[i] = self.get_column_ngram(df, colname, i+1)\n",
    "        return ngrams\n",
    "        \n",
    "    def get_columns_ngrams(self, df, colnames, n=1):\n",
    "        # get all ngrams (1-n) from all columns specified\n",
    "        # dictionary, keys = column names, values = ngram list returned from get_column_ngrams\n",
    "        columns_ngrams = {}\n",
    "        for colname in colnames:\n",
    "            columns_ngrams[colname] = self.get_column_ngrams(df, colname, n)\n",
    "        return columns_ngrams\n",
    "    \n",
    "\n",
    "            \n",
    "nlp = NLP_ngrams()\n",
    "columns_ngrams_1_to_n = nlp.get_columns_ngrams(df, text_cols, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572419c-1df9-4db2-9fe6-e5acec16bc74",
   "metadata": {},
   "source": [
    "Let's briefly explain the previous code results.\n",
    "\n",
    "We have stored a __dictionary__ in the variable __'columns_ngrams_1_to_n'__.\n",
    "\n",
    "This dictionary has the __column names as keys__. In this case we have only __text columns: ['title', 'company', 'location', 'department', 'description', 'company_questions']__\n",
    "\n",
    "On the other hand, the __values for each key is a list__.\n",
    "\n",
    "This __list contains 1 or more dictionaries__. __Each dictionary is the n-gram counts__.\n",
    "\n",
    "__Example:__ key='title', value = [{unigrams dictionary}, {bigrams dictionary}, {trigrams dictionary}]\n",
    "\n",
    "Now, let's take a __closer look to an n-gram dictionary__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fc42aeb-c5a0-42f2-b89e-ff633cbccdeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "'property manager': 199,\n",
      "'support officer': 166,\n",
      "'general manager': 153,\n",
      "'administration officer': 142,\n",
      "'administration assistant': 140,\n",
      "'people culture': 138,\n",
      "'business partner': 138,\n",
      "'customer service': 119,\n",
      "'part time': 116,\n",
      "'human resources': 115,\n",
      "'sales assistant': 115,\n",
      "'support worker': 104,\n",
      "'real estate': 103,\n",
      "'assistant accountant': 98,\n",
      "'accounts payable': 97,\n",
      "'it support': 95,\n",
      "'financial accountant': 90,\n",
      "'finance manager': 88,\n",
      "'medical receptionist': 86,\n",
      "'team member': 86,\n",
      "'executive officer': 83,\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# access bigrams from the column 'title', print only top 20 most common bigrams\n",
    "print(\"{\")\n",
    "for i, (key, value) in enumerate(columns_ngrams_1_to_n['title'][1].items()):\n",
    "    print(\"'\"+key+\"':\", str(value)+\",\")\n",
    "    if i == 20: break\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7c72f03-d6cc-4067-99b0-8b82e2cb56e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "'australian federal police': 34,\n",
      "'recruitment real estate': 27,\n",
      "'sharp carter accounting': 27,\n",
      "'carter accounting clerical': 27,\n",
      "'gough recruitment real': 26,\n",
      "'real estate property': 26,\n",
      "'estate property development': 26,\n",
      "'property development construction': 26,\n",
      "'department communities justice': 25,\n",
      "'aldi stores australia': 25,\n",
      "'hospital health service': 23,\n",
      "'perigon group limited': 23,\n",
      "'department health queensland': 21,\n",
      "'australian government solicitor': 20,\n",
      "'local health district': 19,\n",
      "'eden ritchie recruitment': 19,\n",
      "'allianz australia insurance': 18,\n",
      "'randstad business support': 17,\n",
      "'australian clinical labs': 17,\n",
      "'australian football league': 17,\n",
      "'vincent paul society': 15,\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# access trigrams from the column 'company', print only top 20 most common trigrams\n",
    "print(\"{\")\n",
    "for i, (key, value) in enumerate(columns_ngrams_1_to_n['company'][2].items()):\n",
    "    print(\"'\" + key + \"':\", str(value) + \",\")\n",
    "    if i == 20: break\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8317249-eb18-45fa-851d-c14ca96945a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "'sydney': 2287,\n",
      "'melbourne': 1793,\n",
      "'brisbane': 1372,\n",
      "'coast': 1130,\n",
      "'perth': 778,\n",
      "'north': 566,\n",
      "'west': 473,\n",
      "'south': 464,\n",
      "'adelaide': 455,\n",
      "'newcastle': 340,\n",
      "'central': 324,\n",
      "'gold': 314,\n",
      "'maitland': 256,\n",
      "'hunter': 237,\n",
      "'canberra': 232,\n",
      "'valley': 198,\n",
      "'park': 184,\n",
      "'sunshine': 181,\n",
      "'toowoomba': 177,\n",
      "'wagga': 162,\n",
      "'wollongong': 159,\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# access unigrams from the column 'location', print only top 20 most common unigrams\n",
    "print(\"{\")\n",
    "for i, (key, value) in enumerate(columns_ngrams_1_to_n['location'][0].items()):\n",
    "    print(\"'\" + key + \"':\", str(value) + \",\")\n",
    "    if i == 20: break\n",
    "print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09d3ab-664d-47f1-85bf-2a67ae53bd37",
   "metadata": {},
   "source": [
    "We have seen some examples of our stored n-gram dictionaries (unigrams, bigrams, and trigrams, for all our text columns).\n",
    "\n",
    "Let's go ahead and __count these unigrams, bigrams, and trigrams per column__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d61ff770-fcb2-4236-ba44-463583d005e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': [2885, 8696, 8268],\n",
       " 'company': [4652, 5658, 2687],\n",
       " 'location': [1326, 1751, 1007],\n",
       " 'department': [351, 575, 653],\n",
       " 'description': [103283, 819322, 1433393],\n",
       " 'company_questions': [2762, 7674, 11191]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_columns_ngrams_unique_values(columns_ngrams):\n",
    "    ncolumns = len(columns_ngrams) # number of columns\n",
    "    colnames = columns_ngrams.keys() # column names\n",
    "    nngrams = len(columns_ngrams[list(colnames)[0]]) # number of n-grams stored\n",
    "\n",
    "    # loop through the column names and save the number of keys within each n-gram dictionary\n",
    "    nuniques = {}\n",
    "    for colname in colnames:\n",
    "        nuniques[colname] = [len(ngrams) for ngrams in columns_ngrams[colname]]\n",
    "\n",
    "    return nuniques\n",
    "\n",
    "nuniques = count_columns_ngrams_unique_values(columns_ngrams_1_to_n)\n",
    "nuniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b3e27-a2ec-40ff-9ade-11deaaa84e4a",
   "metadata": {},
   "source": [
    "We have __many n-grams for each column__. The number of n-grams are significantly high.\n",
    "\n",
    "However, on our next step we will __replace the dataset values with the most common n-gram found for that same column__.\n",
    "\n",
    "This will __standardize our unique values and significantly reduce the number of unique values__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b03f6e12-2f85-439b-845c-465224861b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first, let's reset the rows index\n",
    "df.reset_index(inplace=True)\n",
    "del df['index']\n",
    "\n",
    "class NLP_standardization():\n",
    "    \n",
    "    def standardize_column_based_on_ngrams(self, df, colname, ngrams_list, n):\n",
    "        # dictionary of n-grams (unigrams, bigrams, or trigrams)\n",
    "        ngrams = ngrams_list[n-1]\n",
    "        \n",
    "        # iterate through the rows of the column\n",
    "        for i, row in df[[colname]].iterrows():\n",
    "            # make sure value is a string\n",
    "            value = row[0]\n",
    "            if not isinstance(value, str): continue\n",
    "            \n",
    "            # word tokenize the value\n",
    "            tokens = word_tokenize(value)\n",
    "            \n",
    "            # loop through all possible n-grams and look for the most common saved on the dictionary of n-grams\n",
    "            most_common_ngram = \"\", 0\n",
    "            for j in range(len(tokens) - n + 1):\n",
    "                ngram = ' '.join(tokens[j:j+n])\n",
    "                if ngram not in ngrams: continue\n",
    "                if ngrams[ngram] > most_common_ngram[1]:\n",
    "                    most_common_ngram = ngram, ngrams[ngram]\n",
    "            \n",
    "            if most_common_ngram[1] != 0:\n",
    "                df.iloc[i][colname] = most_common_ngram[0]\n",
    "                \n",
    "    def standardize_columns_based_on_ngrams(self, df, colnames, ngrams_list, n_list):\n",
    "        # repeat the previous method process for every column specified as parameter (with their corresponding n of n-gram)\n",
    "        for i, colname in enumerate(colnames):\n",
    "            nlp.standardize_column_based_on_ngrams(df, colname, ngrams_list[colname], n_list[i])\n",
    "        \n",
    "nlp = NLP_standardization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0af0c123-7cf2-4931-935d-68e29d140020",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique values column 'title' before replacing values with bigrams: 5398\n",
      "# Unique values column 'title' after replacing values with bigrams: 2173\n",
      "% Unique values reduction: 59.74 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nunique = df['title'].nunique()\n",
    "print(\"# Unique values column 'title' before replacing values with bigrams:\", nunique)\n",
    "\n",
    "# replace 'title' values with most common bigrams contained in the values\n",
    "nlp.standardize_column_based_on_ngrams(df, 'title', columns_ngrams_1_to_n['title'], 2)\n",
    "new_nunique = df['title'].nunique()\n",
    "print(\"# Unique values column 'title' after replacing values with bigrams:\", new_nunique)\n",
    "print(\"% Unique values reduction:\", round(100 - (new_nunique*100/nunique),2), \"%\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fd17860-c6b2-4326-b3ee-c5f6e8a04405",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique values column 'location' before replacing values with bigrams: 1439\n",
      "# Unique values column 'department' before replacing values with bigrams: 449\n"
     ]
    }
   ],
   "source": [
    "reduce_text_columns = ['location', 'department']\n",
    "n_list = [1, 1]\n",
    "nuniques = [df['location'].nunique(), df['department'].nunique()]\n",
    "print(\"# Unique values column 'location' before replacing values with unigrams:\", nuniques[0])\n",
    "print(\"# Unique values column 'department' before replacing values with unigrams:\", nuniques[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "582d1b57-3eed-4054-97cc-8b8a4c6d3b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique values column 'location' after replacing values with bigrams: 47\n",
      "# Unique values column 'department' after replacing values with bigrams: 29\n",
      "% Unique values reduction: 96.73 %\n",
      "% Unique values reduction: 96.73 %\n"
     ]
    }
   ],
   "source": [
    "nlp.standardize_columns_based_on_ngrams(df, reduce_text_columns, columns_ngrams_1_to_n, n_list)\n",
    "\n",
    "new_nuniques = [df['location'].nunique(), df['department'].nunique()]\n",
    "print(\"# Unique values column 'location' after replacing values with unigrams:\", new_nuniques[0])\n",
    "print(\"# Unique values column 'department' after replacing values with unigrams:\", new_nuniques[1])\n",
    "print(\"% Unique values reduction:\", round(100 - (new_nuniques[0]*100/nuniques[0]),2), \"%\", end=\"\\n\")\n",
    "print(\"% Unique values reduction:\", round(100 - (new_nuniques[0]*100/nuniques[0]),2), \"%\", end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "134c5db7-067c-4c3a-bd17-1aaa1e0f2550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>company_questions</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>support worker</td>\n",
       "      <td>ability gateway</td>\n",
       "      <td>$35.50 per hour [PPT]</td>\n",
       "      <td>wagga</td>\n",
       "      <td>services</td>\n",
       "      <td>Part time</td>\n",
       "      <td>uswe outcome focused ndis service provider bas...</td>\n",
       "      <td>regular access carwhich following statements b...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73909631?type=prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regional manager</td>\n",
       "      <td>catholiccare tasmania</td>\n",
       "      <td>NaN</td>\n",
       "      <td>north</td>\n",
       "      <td>services</td>\n",
       "      <td>Full time</td>\n",
       "      <td>catholiccare tasmania primary social services ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73909232?type=prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>support worker</td>\n",
       "      <td>community</td>\n",
       "      <td>$40 – $44 per hour</td>\n",
       "      <td>northern</td>\n",
       "      <td>services</td>\n",
       "      <td>Full time</td>\n",
       "      <td>community gro inc communitybased nonprofit org...</td>\n",
       "      <td>following statements best describes right work...</td>\n",
       "      <td>2024-02-19</td>\n",
       "      <td>https://www.seek.com.au/job/73832771?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>case manager</td>\n",
       "      <td>open minds</td>\n",
       "      <td>$82k – 84k + super + salary packaging + benefits</td>\n",
       "      <td>coast</td>\n",
       "      <td>services</td>\n",
       "      <td>Full time</td>\n",
       "      <td>case manager coastal supports open minds sunsh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73901240?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>intake worker</td>\n",
       "      <td>centre women</td>\n",
       "      <td>$41 – $42 per hour</td>\n",
       "      <td>brisbane</td>\n",
       "      <td>services</td>\n",
       "      <td>Full time</td>\n",
       "      <td>team culture centre women men services work su...</td>\n",
       "      <td>following statements best describes right work...</td>\n",
       "      <td>2024-02-20</td>\n",
       "      <td>https://www.seek.com.au/job/73861002?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9795</th>\n",
       "      <td>support engineer</td>\n",
       "      <td>fuse technology</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sydney</td>\n",
       "      <td>support</td>\n",
       "      <td>Full time</td>\n",
       "      <td>opportunityas part exciting growth expansion s...</td>\n",
       "      <td>following statements best describes right work...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73930150?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9796</th>\n",
       "      <td>team leader</td>\n",
       "      <td>labourforce</td>\n",
       "      <td>$47 per hour + penalties</td>\n",
       "      <td>sydney</td>\n",
       "      <td>transport</td>\n",
       "      <td>Contract/Temp</td>\n",
       "      <td>client one australias leading manufacturer dis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73870879?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9797</th>\n",
       "      <td>retail assistant</td>\n",
       "      <td>independent living specialists</td>\n",
       "      <td>$31.11 per hour, plus super</td>\n",
       "      <td>sydney</td>\n",
       "      <td>retail</td>\n",
       "      <td>Casual/Vacation</td>\n",
       "      <td>independent living specialists fastgrowing bus...</td>\n",
       "      <td>customer service experiencedo current ndis wor...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73899163?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798</th>\n",
       "      <td>studio assistant</td>\n",
       "      <td>cendre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coast</td>\n",
       "      <td>transport</td>\n",
       "      <td>Full time</td>\n",
       "      <td>cendré revered ecommerce jewellery brand compr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73875587?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9799</th>\n",
       "      <td>support officer</td>\n",
       "      <td>hare forbes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sydney</td>\n",
       "      <td>support</td>\n",
       "      <td>Full time</td>\n",
       "      <td>parramatta locationwork closeknit experienced ...</td>\n",
       "      <td>demonstrated experience diagnosing repairing m...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73868216?type=stan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9800 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title                         company  \\\n",
       "0       support worker                 ability gateway   \n",
       "1     regional manager           catholiccare tasmania   \n",
       "2       support worker                       community   \n",
       "3         case manager                      open minds   \n",
       "4        intake worker                    centre women   \n",
       "...                ...                             ...   \n",
       "9795  support engineer                 fuse technology   \n",
       "9796       team leader                     labourforce   \n",
       "9797  retail assistant  independent living specialists   \n",
       "9798  studio assistant                          cendre   \n",
       "9799   support officer                     hare forbes   \n",
       "\n",
       "                                                salary  location department  \\\n",
       "0                                $35.50 per hour [PPT]     wagga   services   \n",
       "1                                                  NaN     north   services   \n",
       "2                                   $40 – $44 per hour  northern   services   \n",
       "3     $82k – 84k + super + salary packaging + benefits     coast   services   \n",
       "4                                   $41 – $42 per hour  brisbane   services   \n",
       "...                                                ...       ...        ...   \n",
       "9795                                               NaN    sydney    support   \n",
       "9796                          $47 per hour + penalties    sydney  transport   \n",
       "9797                       $31.11 per hour, plus super    sydney     retail   \n",
       "9798                                               NaN     coast  transport   \n",
       "9799                                               NaN    sydney    support   \n",
       "\n",
       "                 type                                        description  \\\n",
       "0           Part time  uswe outcome focused ndis service provider bas...   \n",
       "1           Full time  catholiccare tasmania primary social services ...   \n",
       "2           Full time  community gro inc communitybased nonprofit org...   \n",
       "3           Full time  case manager coastal supports open minds sunsh...   \n",
       "4           Full time  team culture centre women men services work su...   \n",
       "...               ...                                                ...   \n",
       "9795        Full time  opportunityas part exciting growth expansion s...   \n",
       "9796    Contract/Temp  client one australias leading manufacturer dis...   \n",
       "9797  Casual/Vacation  independent living specialists fastgrowing bus...   \n",
       "9798        Full time  cendré revered ecommerce jewellery brand compr...   \n",
       "9799        Full time  parramatta locationwork closeknit experienced ...   \n",
       "\n",
       "                                      company_questions posted_date  \\\n",
       "0     regular access carwhich following statements b...  2024-02-21   \n",
       "1                                                   NaN  2024-02-21   \n",
       "2     following statements best describes right work...  2024-02-19   \n",
       "3                                                   NaN  2024-02-21   \n",
       "4     following statements best describes right work...  2024-02-20   \n",
       "...                                                 ...         ...   \n",
       "9795  following statements best describes right work...  2024-02-21   \n",
       "9796                                                NaN  2024-02-21   \n",
       "9797  customer service experiencedo current ndis wor...  2024-02-21   \n",
       "9798                                                NaN  2024-02-21   \n",
       "9799  demonstrated experience diagnosing repairing m...  2024-02-21   \n",
       "\n",
       "                                                   link  \n",
       "0     https://www.seek.com.au/job/73909631?type=prom...  \n",
       "1     https://www.seek.com.au/job/73909232?type=prom...  \n",
       "2     https://www.seek.com.au/job/73832771?type=stan...  \n",
       "3     https://www.seek.com.au/job/73901240?type=stan...  \n",
       "4     https://www.seek.com.au/job/73861002?type=stan...  \n",
       "...                                                 ...  \n",
       "9795  https://www.seek.com.au/job/73930150?type=stan...  \n",
       "9796  https://www.seek.com.au/job/73870879?type=stan...  \n",
       "9797  https://www.seek.com.au/job/73899163?type=stan...  \n",
       "9798  https://www.seek.com.au/job/73875587?type=stan...  \n",
       "9799  https://www.seek.com.au/job/73868216?type=stan...  \n",
       "\n",
       "[9800 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
