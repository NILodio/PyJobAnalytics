{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd48a22-2567-4638-9d23-c507eb6db083",
   "metadata": {},
   "source": [
    "### Advanced Python AI and ML Tools - Assignment 1\n",
    "\n",
    "__Group Members:__\n",
    "1) Aanal Patel - C0910376\n",
    "2) Bimal Shresta - C0919385\n",
    "3) Danilo Diaz - C0889539\n",
    "4) Ernie Sumoso - C0881591"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d8930-578d-4157-b1c5-368a73ef4b63",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Index\n",
    "- __Step 1. Dataset Description (web scrapped)__\n",
    "- __Step 2. Data Wrangling (cleaning, formatting, structuring, validating)__\n",
    "    - __Step 9. NLP techniques: data cleaning, stopword and puctuation removal, tokenizing, ngrams analysis__\n",
    "- __Step 3. Plotting methods for distribution__\n",
    "- __Step 4. Pandas profiling for EDA (exploratory data analysis)__\n",
    "- __Step 5. Encoding methods, creating new numerical columns__\n",
    "- __Step 6. Outlier identification (with boxplots and IQR)__\n",
    "- __Step 7. Addressing outliers with Quantile-based flooring and capping, Trimming, and Log Transformation__\n",
    "- __Step 8. Unsupervised learning methods__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d3e558-267b-4d55-8b33-4c33c6e77d86",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1. Dataset Description (web scrapped)\n",
    "\n",
    "(Bimal add a description of what you did to web scrap the data here, what is the source and what were your steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a9a780-fd82-442c-bce2-85cd9bc8e1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>job_location</th>\n",
       "      <th>post</th>\n",
       "      <th>job_type</th>\n",
       "      <th>job_desc</th>\n",
       "      <th>company_qns</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>job_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>Level 2/3 Support Engineer</td>\n",
       "      <td>Fuse Technology Pty Ltd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>Help Desk &amp; IT Support (Information &amp; Communic...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>The opportunityAs part of our exciting growth ...</td>\n",
       "      <td>Which of the following statements best describ...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73930150?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>NIGHT SHIFT WAREHOUSE TEAM LEADER WANTED WETHE...</td>\n",
       "      <td>Labourforce</td>\n",
       "      <td>$47 per hour + penalties</td>\n",
       "      <td>Wetherill Park, Sydney NSW</td>\n",
       "      <td>Warehousing, Storage &amp; Distribution (Manufactu...</td>\n",
       "      <td>Contract/Temp</td>\n",
       "      <td>Our client is one of Australia's leading Manuf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73870879?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>Casual Retail Assistant</td>\n",
       "      <td>Independent Living Specialists</td>\n",
       "      <td>$31.11 per hour, plus super</td>\n",
       "      <td>Randwick, Sydney NSW</td>\n",
       "      <td>Retail Assistants (Retail &amp; Consumer Products)</td>\n",
       "      <td>Casual/Vacation</td>\n",
       "      <td>Independent Living Specialists is a fast-growi...</td>\n",
       "      <td>Do you have customer service experience?Do you...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73899163?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>Studio Assistant</td>\n",
       "      <td>Cendre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oxenford, Gold Coast QLD</td>\n",
       "      <td>Pickers &amp; Packers (Manufacturing, Transport &amp; ...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>Cendré is a revered e-commerce jewellery brand...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73875587?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>Junior IT Support Officer</td>\n",
       "      <td>Hare &amp; Forbes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Northmead, Sydney NSW</td>\n",
       "      <td>Help Desk &amp; IT Support (Information &amp; Communic...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>Parramatta locationWork with a close-knit, exp...</td>\n",
       "      <td>Do you have demonstrated experience diagnosing...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73868216?type=stan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              job_title  \\\n",
       "2610                         Level 2/3 Support Engineer   \n",
       "2611  NIGHT SHIFT WAREHOUSE TEAM LEADER WANTED WETHE...   \n",
       "2612                            Casual Retail Assistant   \n",
       "2613                                   Studio Assistant   \n",
       "2614                          Junior IT Support Officer   \n",
       "\n",
       "                             company                       salary  \\\n",
       "2610         Fuse Technology Pty Ltd                          NaN   \n",
       "2611                     Labourforce     $47 per hour + penalties   \n",
       "2612  Independent Living Specialists  $31.11 per hour, plus super   \n",
       "2613                          Cendre                          NaN   \n",
       "2614                   Hare & Forbes                          NaN   \n",
       "\n",
       "                    job_location  \\\n",
       "2610                  Sydney NSW   \n",
       "2611  Wetherill Park, Sydney NSW   \n",
       "2612        Randwick, Sydney NSW   \n",
       "2613    Oxenford, Gold Coast QLD   \n",
       "2614       Northmead, Sydney NSW   \n",
       "\n",
       "                                                   post         job_type  \\\n",
       "2610  Help Desk & IT Support (Information & Communic...        Full time   \n",
       "2611  Warehousing, Storage & Distribution (Manufactu...    Contract/Temp   \n",
       "2612     Retail Assistants (Retail & Consumer Products)  Casual/Vacation   \n",
       "2613  Pickers & Packers (Manufacturing, Transport & ...        Full time   \n",
       "2614  Help Desk & IT Support (Information & Communic...        Full time   \n",
       "\n",
       "                                               job_desc  \\\n",
       "2610  The opportunityAs part of our exciting growth ...   \n",
       "2611  Our client is one of Australia's leading Manuf...   \n",
       "2612  Independent Living Specialists is a fast-growi...   \n",
       "2613  Cendré is a revered e-commerce jewellery brand...   \n",
       "2614  Parramatta locationWork with a close-knit, exp...   \n",
       "\n",
       "                                            company_qns job_posted_date  \\\n",
       "2610  Which of the following statements best describ...      2024-02-21   \n",
       "2611                                                NaN      2024-02-21   \n",
       "2612  Do you have customer service experience?Do you...      2024-02-21   \n",
       "2613                                                NaN      2024-02-21   \n",
       "2614  Do you have demonstrated experience diagnosing...      2024-02-21   \n",
       "\n",
       "                                               job_link  \n",
       "2610  https://www.seek.com.au/job/73930150?type=stan...  \n",
       "2611  https://www.seek.com.au/job/73870879?type=stan...  \n",
       "2612  https://www.seek.com.au/job/73899163?type=stan...  \n",
       "2613  https://www.seek.com.au/job/73875587?type=stan...  \n",
       "2614  https://www.seek.com.au/job/73868216?type=stan...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reading the web scrapped data from CSV file, setting the index column\n",
    "df = pd.read_csv(\"job_data.csv\", index_col=0)\n",
    "\n",
    "# displaying the raw data\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d79c30-ba28-4b8b-a6ee-425ac8107091",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 9800\n",
      "Number of Columns: 10\n",
      "Index(['job_title', 'company', 'salary', 'job_location', 'post', 'job_type',\n",
      "       'job_desc', 'company_qns', 'job_posted_date', 'job_link'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# display the number of rows, columns and the column names\n",
    "def display_shape_and_colnames(df):\n",
    "    print(\"Number of Rows:\", df.shape[0])\n",
    "    print(\"Number of Columns:\", df.shape[1])\n",
    "    print(df.columns)\n",
    "    \n",
    "display_shape_and_colnames(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6837f9d0-3dc3-4986-a9df-337ddc17e9b7",
   "metadata": {},
   "source": [
    "Some of our __column names__ are __redundant__ because we are working with job data.\n",
    "\n",
    "Let's delete the prefix __\"job\"__ from our column names.\n",
    "\n",
    "Some other __column names__ are __abbreviated__ (e.g. \"job_desc\", \"company_qns\").\n",
    "\n",
    "Let's __replace them with full names__ so we can have accurate column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8e2f7a-9d56-4e39-8b51-b145d797646c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>company_questions</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Experienced Support Worker (PPT &amp; CAS)</td>\n",
       "      <td>Ability Gateway</td>\n",
       "      <td>$35.50 per hour [PPT]</td>\n",
       "      <td>Wagga Wagga, Wagga Wagga &amp; Riverina NSW</td>\n",
       "      <td>Aged &amp; Disability Support (Community Services ...</td>\n",
       "      <td>Part time</td>\n",
       "      <td>About usWe are an outcome focused NDIS service...</td>\n",
       "      <td>Do you own or have regular access to a car?Whi...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73909631?type=prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regional Manager - Inspire@HOME</td>\n",
       "      <td>CatholicCare Tasmania</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Launceston, Launceston &amp; North East TAS</td>\n",
       "      <td>Child Welfare, Youth &amp; Family Services (Commun...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>CatholicCare Tasmania is the primary social se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73909232?type=prom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title                company  \\\n",
       "0  Experienced Support Worker (PPT & CAS)        Ability Gateway   \n",
       "1         Regional Manager - Inspire@HOME  CatholicCare Tasmania   \n",
       "\n",
       "                  salary                                 location  \\\n",
       "0  $35.50 per hour [PPT]  Wagga Wagga, Wagga Wagga & Riverina NSW   \n",
       "1                    NaN  Launceston, Launceston & North East TAS   \n",
       "\n",
       "                                          department       type  \\\n",
       "0  Aged & Disability Support (Community Services ...  Part time   \n",
       "1  Child Welfare, Youth & Family Services (Commun...  Full time   \n",
       "\n",
       "                                         description  \\\n",
       "0  About usWe are an outcome focused NDIS service...   \n",
       "1  CatholicCare Tasmania is the primary social se...   \n",
       "\n",
       "                                   company_questions posted_date  \\\n",
       "0  Do you own or have regular access to a car?Whi...  2024-02-21   \n",
       "1                                                NaN  2024-02-21   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.seek.com.au/job/73909631?type=prom...  \n",
       "1  https://www.seek.com.au/job/73909232?type=prom...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_colnames(df):\n",
    "    # delete the prefix \"job_\" on our column names\n",
    "    for column_name in df.columns.to_list():\n",
    "        if column_name.startswith(\"job_\"):\n",
    "            df.rename(columns={column_name : column_name.lstrip(\"job_\")}, inplace=True)\n",
    "\n",
    "    # rename abbreviated column names\n",
    "    df.rename(columns={'desc':'description', 'company_qns':'company_questions', 'post':'department'}, inplace=True)\n",
    "\n",
    "clean_colnames(df)\n",
    "# display clean column names\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf7c020-0c61-4025-8b53-9283211d7ff5",
   "metadata": {},
   "source": [
    "Now let's undestand all of our columns by providing a description to each one:\n",
    "- __title__: title of the posted job\n",
    "- __company__: name of the company that has posted the job\n",
    "- __salary__: salary range for the job, can be defined per hour, monthly, annually, etc.\n",
    "- __location__: geographical location of the job or company\n",
    "- __department__: field or department of the job (e.g. IT, Sales, etc.)\n",
    "- __description__: long description of the job posting\n",
    "- __company_questions__: questions issued by the company to the applicants, according to the post\n",
    "- __posted_date__: format yyyy-mm-dd\n",
    "- __link__: link of the job posting\n",
    "\n",
    "Now that we have a general understanding of our web scrapped data. \n",
    "\n",
    "Let's go ahead to the next step to perform our data wrangling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb7c8b0-5e4d-44bc-9c1f-253acadabe00",
   "metadata": {
    "tags": []
   },
   "source": [
    "### __Step 2.__ Data Wrangling (cleaning, formatting, structuring, validating)\n",
    "### (includes __Step 9.__ NLP techniques: data cleaning, stopword and puctuation removal, tokenizing, ngrams analysis)\n",
    "\n",
    "First of all, we have 9800 rows, however the index values are repetead thrice because of the CSV contents.\n",
    "\n",
    "Let's start by reseting the index to have proper index values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11ba62e5-9313-4bb3-aa44-92bc5f8ca10d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>company_questions</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9795</th>\n",
       "      <td>Level 2/3 Support Engineer</td>\n",
       "      <td>Fuse Technology Pty Ltd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>Help Desk &amp; IT Support (Information &amp; Communic...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>The opportunityAs part of our exciting growth ...</td>\n",
       "      <td>Which of the following statements best describ...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73930150?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9796</th>\n",
       "      <td>NIGHT SHIFT WAREHOUSE TEAM LEADER WANTED WETHE...</td>\n",
       "      <td>Labourforce</td>\n",
       "      <td>$47 per hour + penalties</td>\n",
       "      <td>Wetherill Park, Sydney NSW</td>\n",
       "      <td>Warehousing, Storage &amp; Distribution (Manufactu...</td>\n",
       "      <td>Contract/Temp</td>\n",
       "      <td>Our client is one of Australia's leading Manuf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73870879?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9797</th>\n",
       "      <td>Casual Retail Assistant</td>\n",
       "      <td>Independent Living Specialists</td>\n",
       "      <td>$31.11 per hour, plus super</td>\n",
       "      <td>Randwick, Sydney NSW</td>\n",
       "      <td>Retail Assistants (Retail &amp; Consumer Products)</td>\n",
       "      <td>Casual/Vacation</td>\n",
       "      <td>Independent Living Specialists is a fast-growi...</td>\n",
       "      <td>Do you have customer service experience?Do you...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73899163?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798</th>\n",
       "      <td>Studio Assistant</td>\n",
       "      <td>Cendre</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oxenford, Gold Coast QLD</td>\n",
       "      <td>Pickers &amp; Packers (Manufacturing, Transport &amp; ...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>Cendré is a revered e-commerce jewellery brand...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73875587?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9799</th>\n",
       "      <td>Junior IT Support Officer</td>\n",
       "      <td>Hare &amp; Forbes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Northmead, Sydney NSW</td>\n",
       "      <td>Help Desk &amp; IT Support (Information &amp; Communic...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>Parramatta locationWork with a close-knit, exp...</td>\n",
       "      <td>Do you have demonstrated experience diagnosing...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73868216?type=stan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "9795                         Level 2/3 Support Engineer   \n",
       "9796  NIGHT SHIFT WAREHOUSE TEAM LEADER WANTED WETHE...   \n",
       "9797                            Casual Retail Assistant   \n",
       "9798                                   Studio Assistant   \n",
       "9799                          Junior IT Support Officer   \n",
       "\n",
       "                             company                       salary  \\\n",
       "9795         Fuse Technology Pty Ltd                          NaN   \n",
       "9796                     Labourforce     $47 per hour + penalties   \n",
       "9797  Independent Living Specialists  $31.11 per hour, plus super   \n",
       "9798                          Cendre                          NaN   \n",
       "9799                   Hare & Forbes                          NaN   \n",
       "\n",
       "                        location  \\\n",
       "9795                  Sydney NSW   \n",
       "9796  Wetherill Park, Sydney NSW   \n",
       "9797        Randwick, Sydney NSW   \n",
       "9798    Oxenford, Gold Coast QLD   \n",
       "9799       Northmead, Sydney NSW   \n",
       "\n",
       "                                             department             type  \\\n",
       "9795  Help Desk & IT Support (Information & Communic...        Full time   \n",
       "9796  Warehousing, Storage & Distribution (Manufactu...    Contract/Temp   \n",
       "9797     Retail Assistants (Retail & Consumer Products)  Casual/Vacation   \n",
       "9798  Pickers & Packers (Manufacturing, Transport & ...        Full time   \n",
       "9799  Help Desk & IT Support (Information & Communic...        Full time   \n",
       "\n",
       "                                            description  \\\n",
       "9795  The opportunityAs part of our exciting growth ...   \n",
       "9796  Our client is one of Australia's leading Manuf...   \n",
       "9797  Independent Living Specialists is a fast-growi...   \n",
       "9798  Cendré is a revered e-commerce jewellery brand...   \n",
       "9799  Parramatta locationWork with a close-knit, exp...   \n",
       "\n",
       "                                      company_questions posted_date  \\\n",
       "9795  Which of the following statements best describ...  2024-02-21   \n",
       "9796                                                NaN  2024-02-21   \n",
       "9797  Do you have customer service experience?Do you...  2024-02-21   \n",
       "9798                                                NaN  2024-02-21   \n",
       "9799  Do you have demonstrated experience diagnosing...  2024-02-21   \n",
       "\n",
       "                                                   link  \n",
       "9795  https://www.seek.com.au/job/73930150?type=stan...  \n",
       "9796  https://www.seek.com.au/job/73870879?type=stan...  \n",
       "9797  https://www.seek.com.au/job/73899163?type=stan...  \n",
       "9798  https://www.seek.com.au/job/73875587?type=stan...  \n",
       "9799  https://www.seek.com.au/job/73868216?type=stan...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset the rows index\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276f656-3fad-4023-a673-e95481b2d89c",
   "metadata": {},
   "source": [
    "Now, let's perform some basic analysis on our dataset.\n",
    "\n",
    "We will check the following stats by implementing functions:\n",
    "- missing values per column\n",
    "- duplicated rows\n",
    "- number of unique values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22d8490f-0b52-4dea-ac76-d5fd4e4b5dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Missing Values\n",
      "title                   0\n",
      "company                 0\n",
      "salary               5216\n",
      "location                0\n",
      "department              0\n",
      "type                    0\n",
      "description             0\n",
      "company_questions    5034\n",
      "posted_date             0\n",
      "link                    0\n",
      "dtype: int64\n",
      "\n",
      "% Missing Values\n",
      "title                 0.000000\n",
      "company               0.000000\n",
      "salary               53.224490\n",
      "location              0.000000\n",
      "department            0.000000\n",
      "type                  0.000000\n",
      "description           0.000000\n",
      "company_questions    51.367347\n",
      "posted_date           0.000000\n",
      "link                  0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def check_missing_values(df):\n",
    "    # check for number of missing values per column\n",
    "    print(\"# Missing Values\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    # check for % of missing values\n",
    "    print(\"\\n% Missing Values\")\n",
    "    print(df.isna().mean() * 100)\n",
    "    \n",
    "check_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9511b597-6839-49ed-9fcb-852fce895035",
   "metadata": {},
   "source": [
    "As expected, many job posts do not include a salary range or any information about the salary.\n",
    "\n",
    "It is no surprise that __more than half of our data has missing values for salary__.\n",
    "\n",
    "We also have __more than half missing values for the company questions column__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c435ba31-552a-4e09-898d-80df73894ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Duplicated Values\n",
      "944\n",
      "\n",
      "% Duplicated Values\n",
      "9.63265306122449\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>company_questions</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9772</th>\n",
       "      <td>Pick Packers</td>\n",
       "      <td>Action Workforce</td>\n",
       "      <td>35</td>\n",
       "      <td>Maddington, Perth WA</td>\n",
       "      <td>Warehousing, Storage &amp; Distribution (Manufactu...</td>\n",
       "      <td>Casual/Vacation</td>\n",
       "      <td>Action Workforce are looking for Experienced P...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73901168?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9778</th>\n",
       "      <td>Accounts Person- KALGOORLIE RESIDENTS ONLY</td>\n",
       "      <td>Golden mile cleaning services</td>\n",
       "      <td>$30 – $33.50 per hour</td>\n",
       "      <td>Kalgoorlie, Kalgoorlie, Goldfields &amp; Esperance WA</td>\n",
       "      <td>Administrative Assistants (Administration &amp; Of...</td>\n",
       "      <td>Part time</td>\n",
       "      <td>Job Title: Accounts Person We are currently se...</td>\n",
       "      <td>Which of the following statements best describ...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73908087?type=prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9788</th>\n",
       "      <td>Warehouse Assistant</td>\n",
       "      <td>Omni Recruit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Truganina, Melbourne VIC</td>\n",
       "      <td>Pickers &amp; Packers (Manufacturing, Transport &amp; ...</td>\n",
       "      <td>Casual/Vacation</td>\n",
       "      <td>Business is booming and we are currently seeki...</td>\n",
       "      <td>Do you agree to the privacy policy of Omni Rec...</td>\n",
       "      <td>2024-02-20</td>\n",
       "      <td>https://www.seek.com.au/job/73863322?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9797</th>\n",
       "      <td>Casual Retail Assistant</td>\n",
       "      <td>Independent Living Specialists</td>\n",
       "      <td>$31.11 per hour, plus super</td>\n",
       "      <td>Randwick, Sydney NSW</td>\n",
       "      <td>Retail Assistants (Retail &amp; Consumer Products)</td>\n",
       "      <td>Casual/Vacation</td>\n",
       "      <td>Independent Living Specialists is a fast-growi...</td>\n",
       "      <td>Do you have customer service experience?Do you...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73899163?type=stan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title  \\\n",
       "9772                                Pick Packers   \n",
       "9778  Accounts Person- KALGOORLIE RESIDENTS ONLY   \n",
       "9788                         Warehouse Assistant   \n",
       "9797                     Casual Retail Assistant   \n",
       "\n",
       "                             company                       salary  \\\n",
       "9772                Action Workforce                           35   \n",
       "9778   Golden mile cleaning services        $30 – $33.50 per hour   \n",
       "9788                    Omni Recruit                          NaN   \n",
       "9797  Independent Living Specialists  $31.11 per hour, plus super   \n",
       "\n",
       "                                               location  \\\n",
       "9772                               Maddington, Perth WA   \n",
       "9778  Kalgoorlie, Kalgoorlie, Goldfields & Esperance WA   \n",
       "9788                           Truganina, Melbourne VIC   \n",
       "9797                               Randwick, Sydney NSW   \n",
       "\n",
       "                                             department             type  \\\n",
       "9772  Warehousing, Storage & Distribution (Manufactu...  Casual/Vacation   \n",
       "9778  Administrative Assistants (Administration & Of...        Part time   \n",
       "9788  Pickers & Packers (Manufacturing, Transport & ...  Casual/Vacation   \n",
       "9797     Retail Assistants (Retail & Consumer Products)  Casual/Vacation   \n",
       "\n",
       "                                            description  \\\n",
       "9772  Action Workforce are looking for Experienced P...   \n",
       "9778  Job Title: Accounts Person We are currently se...   \n",
       "9788  Business is booming and we are currently seeki...   \n",
       "9797  Independent Living Specialists is a fast-growi...   \n",
       "\n",
       "                                      company_questions posted_date  \\\n",
       "9772                                                NaN  2024-02-21   \n",
       "9778  Which of the following statements best describ...  2024-02-21   \n",
       "9788  Do you agree to the privacy policy of Omni Rec...  2024-02-20   \n",
       "9797  Do you have customer service experience?Do you...  2024-02-21   \n",
       "\n",
       "                                                   link  \n",
       "9772  https://www.seek.com.au/job/73901168?type=stan...  \n",
       "9778  https://www.seek.com.au/job/73908087?type=prom...  \n",
       "9788  https://www.seek.com.au/job/73863322?type=stan...  \n",
       "9797  https://www.seek.com.au/job/73899163?type=stan...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_duplicated_values(df):\n",
    "    # check for number of duplicated values\n",
    "    print(\"# Duplicated Values\")\n",
    "    print(df.duplicated().sum())\n",
    "    \n",
    "    # check for % of duplicated values\n",
    "    print(\"\\n% Duplicated Values\")\n",
    "    print(df.duplicated().mean() * 100)\n",
    "\n",
    "check_duplicated_values(df)\n",
    "\n",
    "df[df.duplicated()].tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141d4cf-c61e-4847-bc3f-49c84247f2d6",
   "metadata": {},
   "source": [
    "Some considerable amount of our data __(around 9.5%) are duplicated__ rows.\n",
    "\n",
    "This can be __dangerous for analysis__, we have to __deal with these duplicated values__ in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0f48a36-28e8-4ff6-8df1-57c0f9c822f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique Values per Column\n",
      "'title' # of unique values: 5655\n",
      "'company' # of unique values: 4965\n",
      "'salary' # of unique values: 2645\n",
      "'location' # of unique values: 1448\n",
      "'department' # of unique values: 451\n",
      "'type' # of unique values: 8\n",
      "'description' # of unique values: 7958\n",
      "'company_questions' # of unique values: 2730\n",
      "'posted_date' # of unique values: 95\n",
      "'link' # of unique values: 8664\n",
      "\n",
      "% Unique Values per Column\n",
      "'title' % of unique values: 57.7 %\n",
      "'company' % of unique values: 50.66 %\n",
      "'salary' % of unique values: 26.99 %\n",
      "'location' % of unique values: 14.78 %\n",
      "'department' % of unique values: 4.6 %\n",
      "'type' % of unique values: 0.08 %\n",
      "'description' % of unique values: 81.2 %\n",
      "'company_questions' % of unique values: 27.86 %\n",
      "'posted_date' % of unique values: 0.97 %\n",
      "'link' % of unique values: 88.41 %\n"
     ]
    }
   ],
   "source": [
    "def check_nunique_values(df):\n",
    "    # check number of unique values per column\n",
    "    print(\"# Unique Values per Column\")\n",
    "    for col in df.columns:\n",
    "        print(\"'\"+col+\"'\", \"# of unique values:\", df[col].nunique())\n",
    "        \n",
    "    # check % of unique values per column (relative to number of total rows in the dataset)\n",
    "    print(\"\\n% Unique Values per Column\")\n",
    "    for col in df.columns:\n",
    "        print(\"'\"+col+\"'\", \"% of unique values:\", round(df[col].nunique() * 100 / df.shape[0], 2), \"%\")\n",
    "        \n",
    "check_nunique_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e88d6-fc2a-4f32-aa18-43a753474b66",
   "metadata": {},
   "source": [
    "Some of our columns have a __large amount of unique values__.\n",
    "\n",
    "The with vast amount of unique values __(>50% of total rows)__.\n",
    "- title\n",
    "- company\n",
    "- description\n",
    "- link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3423f09-458a-47af-a4e3-e1ddb1371a51",
   "metadata": {},
   "source": [
    "We will start with some basic cleaning applying some NLP methods that includes:\n",
    "- removing punctuation\n",
    "- removing digits\n",
    "- apply lower case to all letters\n",
    "- removing extra whitespaces\n",
    "\n",
    "We will start dealing with the following columns:\n",
    "- title\n",
    "- company\n",
    "- location\n",
    "\n",
    "For this, we will implement classes that __contain NLP methods/techniques__, this code can be re-used later for other columns like 'description', etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9b61c0e-6e6e-44a3-be52-9ddcefbd6691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "# class containing our implemented NLP techniques and methods\n",
    "class NLP():\n",
    "    \n",
    "    # remove all punctuation from a word (string)\n",
    "    def remove_punctuation(self, word):\n",
    "        if not isinstance(word, str): return word\n",
    "        return word.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # remove all digits/numbers from a word (string)\n",
    "    def remove_digits(self, word):\n",
    "        if not isinstance(word, str): return word\n",
    "        return re.sub(r'\\d+', '', word)\n",
    "    \n",
    "    # checks if word is a string and returns lower cased\n",
    "    def lower_word(self, word):\n",
    "        if not isinstance(word, str): return word\n",
    "        return word.lower()\n",
    "\n",
    "    # perform basic operations to clean 1 column of a dataframe\n",
    "    def basic_clean_text_column(self, df, colname):\n",
    "        print(\"Basic cleaning on column '\" + colname + \"':\")\n",
    "        nunique = df[colname].nunique()\n",
    "        print(\"# Unique values before cleaning:\", df[colname].nunique())\n",
    "        for value in df[colname].unique():\n",
    "            # save original value to replace later\n",
    "            og_value = value\n",
    "            \n",
    "            # if we are dealing with a null value, don't modify anything\n",
    "            if value is np.nan: continue\n",
    "            \n",
    "            # remove punctuation from the column value\n",
    "            value = self.remove_punctuation(str(value))\n",
    "            \n",
    "            # remove digits from column value\n",
    "            value = self.remove_digits(value)\n",
    "            \n",
    "            # lower case column value\n",
    "            value = self.lower_word(value)\n",
    "            \n",
    "            # word tokenize the column value\n",
    "            word_tokens = word_tokenize(value)\n",
    "            \n",
    "            # update df value in place\n",
    "            df[colname].replace(og_value, ' '.join(word_tokens), inplace=True)\n",
    "        new_nunique = df[colname].nunique()\n",
    "        print(\"# Unique values after cleaning:\", df[colname].nunique())\n",
    "        print(\"% of unique values reduction:\", round(100 - (new_nunique*100/nunique),2), \"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d6c780-abb2-4a8d-b79a-3a64c2f89fdf",
   "metadata": {},
   "source": [
    "Now that we have implemented a class for our methods,\n",
    "\n",
    "let's go ahead and __apply a basic cleaning on all our columns__.\n",
    "\n",
    "Then, we can __compare values before vs after cleaning__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73deb158-a819-4850-8576-6ddbcc24a1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic cleaning on column 'title':\n",
      "# Unique values before cleaning: 5655\n",
      "# Unique values after cleaning: 5541\n",
      "% of unique values reduction: 2.02 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>clean title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Experienced Support Worker (PPT &amp; CAS)</td>\n",
       "      <td>experienced support worker ppt cas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Regional Manager - Inspire@HOME</td>\n",
       "      <td>regional manager inspirehome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Family Support Worker</td>\n",
       "      <td>family support worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CPS Case Manager</td>\n",
       "      <td>cps case manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Intake Worker</td>\n",
       "      <td>intake worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9795</th>\n",
       "      <td>Level 2/3 Support Engineer</td>\n",
       "      <td>level support engineer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9796</th>\n",
       "      <td>NIGHT SHIFT WAREHOUSE TEAM LEADER WANTED WETHE...</td>\n",
       "      <td>night shift warehouse team leader wanted wethe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9797</th>\n",
       "      <td>Casual Retail Assistant</td>\n",
       "      <td>casual retail assistant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798</th>\n",
       "      <td>Studio Assistant</td>\n",
       "      <td>studio assistant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9799</th>\n",
       "      <td>Junior IT Support Officer</td>\n",
       "      <td>junior it support officer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                Experienced Support Worker (PPT & CAS)   \n",
       "1                       Regional Manager - Inspire@HOME   \n",
       "2                                 Family Support Worker   \n",
       "3                                      CPS Case Manager   \n",
       "4                                         Intake Worker   \n",
       "...                                                 ...   \n",
       "9795                         Level 2/3 Support Engineer   \n",
       "9796  NIGHT SHIFT WAREHOUSE TEAM LEADER WANTED WETHE...   \n",
       "9797                            Casual Retail Assistant   \n",
       "9798                                   Studio Assistant   \n",
       "9799                          Junior IT Support Officer   \n",
       "\n",
       "                                            clean title  \n",
       "0                    experienced support worker ppt cas  \n",
       "1                          regional manager inspirehome  \n",
       "2                                 family support worker  \n",
       "3                                      cps case manager  \n",
       "4                                         intake worker  \n",
       "...                                                 ...  \n",
       "9795                             level support engineer  \n",
       "9796  night shift warehouse team leader wanted wethe...  \n",
       "9797                            casual retail assistant  \n",
       "9798                                   studio assistant  \n",
       "9799                          junior it support officer  \n",
       "\n",
       "[9800 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_and_compare_column(df, colname):\n",
    "    # save raw title data into a new dataframe just to compare before vs after cleaning\n",
    "    df_compare = df[[colname]].copy()\n",
    "\n",
    "    # perform the basic cleaning on the title column\n",
    "    nlp = NLP()\n",
    "    nlp.basic_clean_text_column(df, colname)\n",
    "\n",
    "    # compare before vs after\n",
    "    df_compare[\"clean \"+colname] = df[colname]\n",
    "    display(df_compare)\n",
    "\n",
    "clean_and_compare_column(df, 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56efc01-a23d-4b50-ae8a-bc3979da51b6",
   "metadata": {},
   "source": [
    "After this __1st experiment__ of __cleaning the 'title' column__ we notice that we have __reduced the number of unique values by 114__.\n",
    "\n",
    "Which is equivalent of aproximately __2% of the total unique values__, __not a significant reduction__.\n",
    "\n",
    "However, we have considerably clean our raw texts, and this will allow us to apply further NLP techniques that will have better results.\n",
    "\n",
    "Let's also __apply the same basic cleaning on the other 2 columns:__\n",
    "- company\n",
    "- location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76605134-4666-4978-9972-8697ac556f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic cleaning on column 'company':\n",
      "# Unique values before cleaning: 4965\n",
      "# Unique values after cleaning: 4965\n",
      "% of unique values reduction: 0.0 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>clean company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ability Gateway</td>\n",
       "      <td>ability gateway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatholicCare Tasmania</td>\n",
       "      <td>catholiccare tasmania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Community Gro</td>\n",
       "      <td>community gro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Open Minds</td>\n",
       "      <td>open minds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Centre for Women &amp; Co.</td>\n",
       "      <td>the centre for women co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9795</th>\n",
       "      <td>Fuse Technology Pty Ltd</td>\n",
       "      <td>fuse technology pty ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9796</th>\n",
       "      <td>Labourforce</td>\n",
       "      <td>labourforce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9797</th>\n",
       "      <td>Independent Living Specialists</td>\n",
       "      <td>independent living specialists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798</th>\n",
       "      <td>Cendre</td>\n",
       "      <td>cendre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9799</th>\n",
       "      <td>Hare &amp; Forbes</td>\n",
       "      <td>hare forbes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             company                   clean company\n",
       "0                    Ability Gateway                 ability gateway\n",
       "1              CatholicCare Tasmania           catholiccare tasmania\n",
       "2                      Community Gro                   community gro\n",
       "3                         Open Minds                      open minds\n",
       "4         The Centre for Women & Co.         the centre for women co\n",
       "...                              ...                             ...\n",
       "9795         Fuse Technology Pty Ltd         fuse technology pty ltd\n",
       "9796                     Labourforce                     labourforce\n",
       "9797  Independent Living Specialists  independent living specialists\n",
       "9798                          Cendre                          cendre\n",
       "9799                   Hare & Forbes                     hare forbes\n",
       "\n",
       "[9800 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic cleaning on column 'location':\n",
      "# Unique values before cleaning: 1448\n",
      "# Unique values after cleaning: 1448\n",
      "% of unique values reduction: 0.0 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>clean location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wagga Wagga, Wagga Wagga &amp; Riverina NSW</td>\n",
       "      <td>wagga wagga wagga wagga riverina nsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Launceston, Launceston &amp; North East TAS</td>\n",
       "      <td>launceston launceston north east tas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Townsville, Northern QLD</td>\n",
       "      <td>townsville northern qld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nambour, Sunshine Coast QLD</td>\n",
       "      <td>nambour sunshine coast qld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Underwood, Brisbane QLD</td>\n",
       "      <td>underwood brisbane qld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9795</th>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>sydney nsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9796</th>\n",
       "      <td>Wetherill Park, Sydney NSW</td>\n",
       "      <td>wetherill park sydney nsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9797</th>\n",
       "      <td>Randwick, Sydney NSW</td>\n",
       "      <td>randwick sydney nsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9798</th>\n",
       "      <td>Oxenford, Gold Coast QLD</td>\n",
       "      <td>oxenford gold coast qld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9799</th>\n",
       "      <td>Northmead, Sydney NSW</td>\n",
       "      <td>northmead sydney nsw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     location  \\\n",
       "0     Wagga Wagga, Wagga Wagga & Riverina NSW   \n",
       "1     Launceston, Launceston & North East TAS   \n",
       "2                    Townsville, Northern QLD   \n",
       "3                 Nambour, Sunshine Coast QLD   \n",
       "4                     Underwood, Brisbane QLD   \n",
       "...                                       ...   \n",
       "9795                               Sydney NSW   \n",
       "9796               Wetherill Park, Sydney NSW   \n",
       "9797                     Randwick, Sydney NSW   \n",
       "9798                 Oxenford, Gold Coast QLD   \n",
       "9799                    Northmead, Sydney NSW   \n",
       "\n",
       "                            clean location  \n",
       "0     wagga wagga wagga wagga riverina nsw  \n",
       "1     launceston launceston north east tas  \n",
       "2                  townsville northern qld  \n",
       "3               nambour sunshine coast qld  \n",
       "4                   underwood brisbane qld  \n",
       "...                                    ...  \n",
       "9795                            sydney nsw  \n",
       "9796             wetherill park sydney nsw  \n",
       "9797                   randwick sydney nsw  \n",
       "9798               oxenford gold coast qld  \n",
       "9799                  northmead sydney nsw  \n",
       "\n",
       "[9800 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the remaining text columns that we need to perform a basic clean\n",
    "text_cols = ['company', 'location']\n",
    "\n",
    "# implement a function to perform the cleaning on these columns\n",
    "def clean_and_compare_columns(df, cols):\n",
    "    for colname in cols:\n",
    "        clean_and_compare_column(df, colname)\n",
    "\n",
    "# call the implemented function\n",
    "clean_and_compare_columns(df, text_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a36db8-4d5f-45a3-8114-86353f62b8ee",
   "metadata": {},
   "source": [
    "Let's take a look to the entire __dataframe__ in the __current clean version__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19515e7e-9668-4c0c-ab01-e785e61810ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>company_questions</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>experienced support worker ppt cas</td>\n",
       "      <td>ability gateway</td>\n",
       "      <td>$35.50 per hour [PPT]</td>\n",
       "      <td>wagga wagga wagga wagga riverina nsw</td>\n",
       "      <td>Aged &amp; Disability Support (Community Services ...</td>\n",
       "      <td>Part time</td>\n",
       "      <td>About usWe are an outcome focused NDIS service...</td>\n",
       "      <td>Do you own or have regular access to a car?Whi...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73909631?type=prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regional manager inspirehome</td>\n",
       "      <td>catholiccare tasmania</td>\n",
       "      <td>NaN</td>\n",
       "      <td>launceston launceston north east tas</td>\n",
       "      <td>Child Welfare, Youth &amp; Family Services (Commun...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>CatholicCare Tasmania is the primary social se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73909232?type=prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family support worker</td>\n",
       "      <td>community gro</td>\n",
       "      <td>$40 – $44 per hour</td>\n",
       "      <td>townsville northern qld</td>\n",
       "      <td>Child Welfare, Youth &amp; Family Services (Commun...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>Community Gro Inc is a community-based non-pro...</td>\n",
       "      <td>Which of the following statements best describ...</td>\n",
       "      <td>2024-02-19</td>\n",
       "      <td>https://www.seek.com.au/job/73832771?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cps case manager</td>\n",
       "      <td>open minds</td>\n",
       "      <td>$82k – 84k + super + salary packaging + benefits</td>\n",
       "      <td>nambour sunshine coast qld</td>\n",
       "      <td>Community Development (Community Services &amp; De...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>As a Case Manager for Coastal Supports at Open...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73901240?type=stan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>intake worker</td>\n",
       "      <td>the centre for women co</td>\n",
       "      <td>$41 – $42 per hour</td>\n",
       "      <td>underwood brisbane qld</td>\n",
       "      <td>Child Welfare, Youth &amp; Family Services (Commun...</td>\n",
       "      <td>Full time</td>\n",
       "      <td>About Us and Our Team Culture   At The Centre ...</td>\n",
       "      <td>Which of the following statements best describ...</td>\n",
       "      <td>2024-02-20</td>\n",
       "      <td>https://www.seek.com.au/job/73861002?type=stan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title                  company  \\\n",
       "0  experienced support worker ppt cas          ability gateway   \n",
       "1        regional manager inspirehome    catholiccare tasmania   \n",
       "2               family support worker            community gro   \n",
       "3                    cps case manager               open minds   \n",
       "4                       intake worker  the centre for women co   \n",
       "\n",
       "                                             salary  \\\n",
       "0                             $35.50 per hour [PPT]   \n",
       "1                                               NaN   \n",
       "2                                $40 – $44 per hour   \n",
       "3  $82k – 84k + super + salary packaging + benefits   \n",
       "4                                $41 – $42 per hour   \n",
       "\n",
       "                               location  \\\n",
       "0  wagga wagga wagga wagga riverina nsw   \n",
       "1  launceston launceston north east tas   \n",
       "2               townsville northern qld   \n",
       "3            nambour sunshine coast qld   \n",
       "4                underwood brisbane qld   \n",
       "\n",
       "                                          department       type  \\\n",
       "0  Aged & Disability Support (Community Services ...  Part time   \n",
       "1  Child Welfare, Youth & Family Services (Commun...  Full time   \n",
       "2  Child Welfare, Youth & Family Services (Commun...  Full time   \n",
       "3  Community Development (Community Services & De...  Full time   \n",
       "4  Child Welfare, Youth & Family Services (Commun...  Full time   \n",
       "\n",
       "                                         description  \\\n",
       "0  About usWe are an outcome focused NDIS service...   \n",
       "1  CatholicCare Tasmania is the primary social se...   \n",
       "2  Community Gro Inc is a community-based non-pro...   \n",
       "3  As a Case Manager for Coastal Supports at Open...   \n",
       "4  About Us and Our Team Culture   At The Centre ...   \n",
       "\n",
       "                                   company_questions posted_date  \\\n",
       "0  Do you own or have regular access to a car?Whi...  2024-02-21   \n",
       "1                                                NaN  2024-02-21   \n",
       "2  Which of the following statements best describ...  2024-02-19   \n",
       "3                                                NaN  2024-02-21   \n",
       "4  Which of the following statements best describ...  2024-02-20   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.seek.com.au/job/73909631?type=prom...  \n",
       "1  https://www.seek.com.au/job/73909232?type=prom...  \n",
       "2  https://www.seek.com.au/job/73832771?type=stan...  \n",
       "3  https://www.seek.com.au/job/73901240?type=stan...  \n",
       "4  https://www.seek.com.au/job/73861002?type=stan...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display our current dataframe version\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85414198-d252-42e7-aa85-323d9c671c52",
   "metadata": {},
   "source": [
    "We have seen __lots of stopwords accross our dataset__.\n",
    "\n",
    "Our next step for cleaning is to remove all those stopwords.\n",
    "\n",
    "However, we must __pay attention to certain words that have important meaning and at the same time are considered stopwords__.\n",
    "\n",
    "- __Example:__ The most common meaning of __\"it\"__ is considered a stopword. However, \"it\" in job postings may refer to \"Information Technologies\".\n",
    "\n",
    "Let's start by identifying words of 1, 2, and 3 characters long, so we can __identify which ones to remove, and which ones to keep__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c7fdc83-696a-4b3a-b235-05da92f397a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words of length 1 to 3 on column 'title'\n",
      "- Words Length 1\n",
      "['a', 'd', 'f', 'i', 'k', 'l', 'm', 'n', 'p', 's', 't', 'v', 'w', 'x', 'y', '–', '’', '💡', '🤝']\n",
      "- Words Length 2\n",
      "['ah', 'ai', 'am', 'an', 'ao', 'ap', 'ar', 'as', 'at', 'au', 'av', 'ba', 'bb', 'bi', 'bp', 'ca', 'cc', 'ci', 'co', 'cx', 'dc', 'do', 'ds', 'ea', 'el', 'er', 'fm', 'fq', 'ft', 'gc', 'gm', 'go', 'gp', 'hc', 'hm', 'hr', 'ic', 'in', 'it', 'iv', 'ld', 'le', 'lf', 'lo', 'ma', 'mc', 'md', 'mq', 'mr', 'ms', 'mt', 'my', 'nd', 'no', 'nt', 'od', 'of', 'on', 'oo', 'or', 'ot', 'pa', 'pc', 'ph', 'pm', 'po', 'pt', 'pw', 'px', 'qa', 'qc', 'rd', 're', 'rn', 'sa', 'sc', 'sr', 'st', 'sw', 'to', 'tq', 'up', 'us', 'vp', 'wa', 'we', 'yr', '⚽️']\n",
      "- Words Length 3\n",
      "['abn', 'acm', 'act', 'age', 'ags', 'aid', 'ain', 'air', 'ald', 'alh', 'ali', 'all', 'ame', 'and', 'anz', 'aod', 'app', 'aps', 'apy', 'arc', 'are', 'aso', 'asx', 'atm', 'aus', 'aws', 'bar', 'bas', 'bay', 'bdm', 'bft', 'bgs', 'bid', 'bms', 'bom', 'box', 'bus', 'bws', 'cad', 'car', 'cas', 'cbd', 'ccs', 'cdc', 'ceo', 'cfo', 'cmt', 'cmy', 'cnc', 'cns', 'coo', 'cpc', 'cps', 'crk', 'crm', 'csl', 'ctp', 'daf', 'day', 'dev', 'dfo', 'dfv', 'div', 'dna', 'dog', 'dry', 'due', 'eca', 'ecm', 'egm', 'eho', 'ehs', 'elc', 'elm', 'emu', 'end', 'eoi', 'esd', 'esg', 'eso', 'euc', 'exp', 'far', 'fit', 'fix', 'fld', 'foi', 'fom', 'for', 'fpa', 'ftc', 'fte', 'fun', 'gap', 'gas', 'gcf', 'get', 'gin', 'gis', 'gmp', 'gpc', 'gym', 'hbc', 'hcp', 'her', 'his', 'hsp', 'hub', 'icp', 'ict', 'iga', 'imc', 'ims', 'inc', 'ion', 'isa', 'iso', 'itc', 'its', 'itt', 'ivf', 'jay', 'jmf', 'job', 'key', 'kit', 'lab', 'law', 'llc', 'lms', 'ltd', 'lvl', 'mcv', 'mep', 'mgr', 'mid', 'moe', 'msp', 'mth', 'net', 'new', 'nfp', 'ngs', 'nmr', 'nnw', 'non', 'now', 'nsw', 'nth', 'num', 'occ', 'off', 'one', 'ops', 'org', 'otc', 'ote', 'our', 'out', 'pae', 'pay', 'pcp', 'per', 'pet', 'phd', 'pmo', 'png', 'pos', 'ppt', 'pqe', 'pre', 'psu', 'pts', 'pty', 'qld', 'qsr', 'rab', 'ras', 'ray', 'rda', 'rep', 'rpd', 'rto', 'sap', 'sea', 'seo', 'ses', 'set', 'sil', 'sme', 'smp', 'sor', 'spa', 'spt', 'stp', 'sub', 'syd', 'tas', 'tax', 'tcs', 'tea', 'the', 'tmp', 'top', 'trc', 'try', 'ttw', 'two', 'uni', 'ute', 'van', 'vce', 'veg', 'vic', 'vps', 'web', 'wet', 'wfa', 'wfh', 'whs', 'whv', 'wmc', 'woy', 'yha', 'you']\n"
     ]
    }
   ],
   "source": [
    "# return a list of lists, each list will contain the words of length 1, 2, 3... n\n",
    "def identify_words_len_1_to_n(df, colname, n):\n",
    "    # set n number of empty lists\n",
    "    words = [[] for _ in range(n)]\n",
    "    \n",
    "    # loop through unique values of the column\n",
    "    for value in df[colname].unique():\n",
    "        # if it's not a string, go to the next value\n",
    "        if not isinstance(value, str): continue\n",
    "        \n",
    "        # tokenize the value, loop through the words, if the word length its in range, add them to corresponding list\n",
    "        tokens = word_tokenize(value)\n",
    "        for word in tokens:\n",
    "            if len(word) <= n:\n",
    "                words[len(word)-1].append(word)\n",
    "                \n",
    "    # delete repeated values in the lists and sort them\n",
    "    words_len_1_to_n = [sorted(list(set(words_sublist))) for words_sublist in words]\n",
    "    \n",
    "    # print the results (each list)\n",
    "    print(\"Words of length 1 to\", n, \"on column '\"+colname+\"'\")\n",
    "    for i in range(n):\n",
    "        print(\"- Words Length\", i+1)\n",
    "        print(words_len_1_to_n[i])\n",
    "    return words_len_1_to_n\n",
    "\n",
    "words_len_1_to_3 = identify_words_len_1_to_n(df, 'title', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee8744-1d68-4a7c-8a6a-9a03916e76d3",
   "metadata": {},
   "source": [
    "For our column title all words length 1 need to be removed, as they don't bring any value to our analysis.\n",
    "\n",
    "The only 1-length string that will not be removed is the apostrophe to keep word consistency.\n",
    "- __’__ : apostrophe\n",
    "\n",
    "From our 2 length words, we will remove most of them except for the following common job accronyms:\n",
    "- __hr__ : Human Resources\n",
    "- __it__: Information Technology\n",
    "\n",
    "From the 3 length words, again we will remove most of them except for the following:\n",
    "- __ceo__: Chief Executive Officer\n",
    "- __cfo__: Chief Financial Officer\n",
    "- __aws__: Amazon Web Services\n",
    "- __pmo__: Project Management Office\n",
    "- __pcp__: Primary Care Physician\n",
    "- __crm__: Customer Relationship Management\n",
    "- __sap__: System Applications (ERP leader)\n",
    "- __app__: application\n",
    "- __dev__: developer\n",
    "- __lab__: laboratory\n",
    "- __web__: internet\n",
    "- __law__: self-explanatory\n",
    "\n",
    "Let's perform the same operation with the rest of the text columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "931825ec-bca3-4fa8-9704-a2034083630c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text columns: ['title', 'company', 'location']\n",
      "Words max length: [3, 2, 1]\n",
      "\n",
      "Words of length 1 to 3 on column 'title'\n",
      "- Words Length 1\n",
      "['a', 'd', 'f', 'i', 'k', 'l', 'm', 'n', 'p', 's', 't', 'v', 'w', 'x', 'y', '–', '’', '💡', '🤝']\n",
      "- Words Length 2\n",
      "['ah', 'ai', 'am', 'an', 'ao', 'ap', 'ar', 'as', 'at', 'au', 'av', 'ba', 'bb', 'bi', 'bp', 'ca', 'cc', 'ci', 'co', 'cx', 'dc', 'do', 'ds', 'ea', 'el', 'er', 'fm', 'fq', 'ft', 'gc', 'gm', 'go', 'gp', 'hc', 'hm', 'hr', 'ic', 'in', 'it', 'iv', 'ld', 'le', 'lf', 'lo', 'ma', 'mc', 'md', 'mq', 'mr', 'ms', 'mt', 'my', 'nd', 'no', 'nt', 'od', 'of', 'on', 'oo', 'or', 'ot', 'pa', 'pc', 'ph', 'pm', 'po', 'pt', 'pw', 'px', 'qa', 'qc', 'rd', 're', 'rn', 'sa', 'sc', 'sr', 'st', 'sw', 'to', 'tq', 'up', 'us', 'vp', 'wa', 'we', 'yr', '⚽️']\n",
      "- Words Length 3\n",
      "['abn', 'acm', 'act', 'age', 'ags', 'aid', 'ain', 'air', 'ald', 'alh', 'ali', 'all', 'ame', 'and', 'anz', 'aod', 'app', 'aps', 'apy', 'arc', 'are', 'aso', 'asx', 'atm', 'aus', 'aws', 'bar', 'bas', 'bay', 'bdm', 'bft', 'bgs', 'bid', 'bms', 'bom', 'box', 'bus', 'bws', 'cad', 'car', 'cas', 'cbd', 'ccs', 'cdc', 'ceo', 'cfo', 'cmt', 'cmy', 'cnc', 'cns', 'coo', 'cpc', 'cps', 'crk', 'crm', 'csl', 'ctp', 'daf', 'day', 'dev', 'dfo', 'dfv', 'div', 'dna', 'dog', 'dry', 'due', 'eca', 'ecm', 'egm', 'eho', 'ehs', 'elc', 'elm', 'emu', 'end', 'eoi', 'esd', 'esg', 'eso', 'euc', 'exp', 'far', 'fit', 'fix', 'fld', 'foi', 'fom', 'for', 'fpa', 'ftc', 'fte', 'fun', 'gap', 'gas', 'gcf', 'get', 'gin', 'gis', 'gmp', 'gpc', 'gym', 'hbc', 'hcp', 'her', 'his', 'hsp', 'hub', 'icp', 'ict', 'iga', 'imc', 'ims', 'inc', 'ion', 'isa', 'iso', 'itc', 'its', 'itt', 'ivf', 'jay', 'jmf', 'job', 'key', 'kit', 'lab', 'law', 'llc', 'lms', 'ltd', 'lvl', 'mcv', 'mep', 'mgr', 'mid', 'moe', 'msp', 'mth', 'net', 'new', 'nfp', 'ngs', 'nmr', 'nnw', 'non', 'now', 'nsw', 'nth', 'num', 'occ', 'off', 'one', 'ops', 'org', 'otc', 'ote', 'our', 'out', 'pae', 'pay', 'pcp', 'per', 'pet', 'phd', 'pmo', 'png', 'pos', 'ppt', 'pqe', 'pre', 'psu', 'pts', 'pty', 'qld', 'qsr', 'rab', 'ras', 'ray', 'rda', 'rep', 'rpd', 'rto', 'sap', 'sea', 'seo', 'ses', 'set', 'sil', 'sme', 'smp', 'sor', 'spa', 'spt', 'stp', 'sub', 'syd', 'tas', 'tax', 'tcs', 'tea', 'the', 'tmp', 'top', 'trc', 'try', 'ttw', 'two', 'uni', 'ute', 'van', 'vce', 'veg', 'vic', 'vps', 'web', 'wet', 'wfa', 'wfh', 'whs', 'whv', 'wmc', 'woy', 'yha', 'you']\n",
      "\n",
      "\n",
      "Words of length 1 to 2 on column 'company'\n",
      "- Words Length 1\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', '–', '’']\n",
      "- Words Length 2\n",
      "['ag', 'ah', 'ak', 'as', 'at', 'au', 'aw', 'ay', 'be', 'bp', 'br', 'bt', 'by', 'cg', 'ck', 'co', 'cs', 'da', 'de', 'do', 'dp', 'dr', 'ds', 'dt', 'dw', 'eg', 'em', 'ey', 'fc', 'fi', 'ft', 'ge', 'go', 'gp', 'hb', 'hd', 'hi', 'ho', 'hr', 'ic', 'in', 'ip', 'iq', 'it', 'jb', 'jd', 'jk', 'js', 'jv', 'ke', 'kh', 'ks', 'la', 'le', 'lf', 'lg', 'li', 'lj', 'lk', 'ly', 'mb', 'me', 'mj', 'mk', 'mq', 'mr', 'ms', 'mt', 'mv', 'mw', 'my', 'na', 'nc', 'nh', 'nl', 'no', 'nq', 'nt', 'nx', 'nz', 'of', 'on', 'op', 'oz', 'pc', 'pe', 'pl', 'pr', 'ps', 'pv', 'qb', 'qt', 'rd', 're', 'rk', 'rp', 'sa', 'sb', 'sc', 'se', 'sk', 'sm', 'st', 'ta', 'td', 'th', 'tj', 'tm', 'to', 'ts', 'ty', 'ua', 'up', 'uq', 'us', 'uu', 'va', 'wa', 'wb', 'wd', 'we', 'wh', 'ws', 'xm', 'xo', 'yd']\n",
      "\n",
      "\n",
      "Words of length 1 to 1 on column 'location'\n",
      "- Words Length 1\n",
      "['m']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def identify_words_len_1_to_n_columns(df, text_columns, ns):\n",
    "    # loop through the specified columns and identify the words of length 1 to n\n",
    "    words_per_col = []\n",
    "    for i, colname in enumerate(text_columns):\n",
    "        words_per_col.append(identify_words_len_1_to_n(df, colname, ns[i]))\n",
    "        print(\"\\n\")\n",
    "    return words_per_col\n",
    "\n",
    "# define the word lengths per text column\n",
    "text_cols = ['title', 'company', 'location']\n",
    "word_max_lens = [3, 2, 1]\n",
    "print(\"Text columns:\", text_cols, end='\\n')\n",
    "print(\"Finding words of length:\", word_max_lens, end='\\n\\n')\n",
    "words_per_col = identify_words_len_1_to_n_columns(df, text_cols, word_max_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca5431-4393-4836-9162-022294df3078",
   "metadata": {},
   "source": [
    "Now that we have identified more words to remove, let's __implement a function that removes all stopwords__ on top of the extra ones.\n",
    "\n",
    "Let's also keep in mind the __list of values that should not be removed__ (from the same analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42d19419-9f71-491b-94ad-a9b50f0599ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import flatten # convert nested list into 1D list\n",
    "\n",
    "def set_additional_stopwords(words_per_col):\n",
    "    # set our additional stopwords making use of the identified 1 to 3 length words for each column\n",
    "    additionals = []\n",
    "    for column_words in words_per_col:\n",
    "        # make sure we only have unique values by using set\n",
    "        additionals.append(list(set(flatten(column_words))))\n",
    "    return additionals\n",
    "\n",
    "additionals = set_additional_stopwords(words_per_col) # pass our list of lists defined in the previous code block\n",
    "\n",
    "# set the exceptions manually based on our word length analysis\n",
    "exceptions = ['’', 'hr', 'it', 'ceo', 'cfo', 'aws', 'pmo', 'pcp', 'crm', 'sap', 'app', 'dev', 'lab', 'web', 'law']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9927e49-5f19-4935-a839-952683b4c775",
   "metadata": {},
   "source": [
    "Now let's __implement a new class that stores our stopwords removal methods__.\n",
    "\n",
    "We will use this class to perform the stopwords removal __taking into account our additional stopwords and exceptions__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f8d54a3-735f-44bf-ade3-76d45eacc377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stopwords on column 'title'\n",
      "# Unique values with stopwords: 5541\n",
      "# Unique values without stopwords: 5398\n",
      "% of unique values reduction: 2.58 %\n",
      "\n",
      "Removing stopwords on column 'company'\n",
      "# Unique values with stopwords: 4965\n",
      "# Unique values without stopwords: 4925\n",
      "% of unique values reduction: 0.81 %\n",
      "\n",
      "Removing stopwords on column 'location'\n",
      "# Unique values with stopwords: 1448\n",
      "# Unique values without stopwords: 1448\n",
      "% of unique values reduction: 0.0 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NLP_stopwords():\n",
    "    \n",
    "    def remove_stopwords_columns(self, df, colnames, additionals=[], exceptions=[]):\n",
    "        # remove stopwords from a set of textual columns passed as parameters\n",
    "        if additionals == []:\n",
    "            additionals = [[] for _ in range(len(colnames))]\n",
    "        if len(colnames) != len(additionals):\n",
    "            raise Exception(\"Column names length must be equal to the additional stop words.\")\n",
    "\n",
    "        # remove stopwords on specified columns\n",
    "        for i, colname in enumerate(colnames):\n",
    "            self.remove_stopwords_column(df, colname, additionals[i], exceptions)\n",
    "\n",
    "    def remove_stopwords_column(self, df, colname, additional=[], exceptions=[]):\n",
    "        print(\"Removing stopwords on column '\" + colname + \"'\")\n",
    "        nunique = df[colname].nunique()\n",
    "        print(\"# Unique values with stopwords:\", df[colname].nunique())        \n",
    "        \n",
    "        # loop through unique values of the column\n",
    "        for value in df[colname].unique():\n",
    "            # make sure the value is a string\n",
    "            if not isinstance(value, str): continue\n",
    "            \n",
    "            # tokenize the unique column value\n",
    "            tokens = word_tokenize(value)\n",
    "\n",
    "            # remove stopwords\n",
    "            self.remove_stopwords_tokens(tokens, additional, exceptions)\n",
    "\n",
    "            # update df value in place\n",
    "            df[colname].replace(value, ' '.join(tokens), inplace=True)\n",
    "        \n",
    "        new_nunique = df[colname].nunique()\n",
    "        print(\"# Unique values without stopwords:\", df[colname].nunique())\n",
    "        print(\"% of unique values reduction:\", round(100 - (new_nunique*100/nunique),2), \"%\", end=\"\\n\\n\")\n",
    "\n",
    "    def remove_stopwords_tokens(self, tokens, additional=[], exceptions=[]):\n",
    "        # remove stopwords on a list of word tokens\n",
    "        i = 0\n",
    "        # add the additional parameter stopwords\n",
    "        total_stopwords = stopwords.words('english') + additional\n",
    "        while i < len(tokens):\n",
    "            word = tokens[i]\n",
    "            # if the word is in exceptions, don't remove it\n",
    "            if word in total_stopwords and word not in exceptions:\n",
    "                tokens.pop(i)\n",
    "                i -= 1\n",
    "            i += 1\n",
    "\n",
    "nlp = NLP_stopwords()\n",
    "text_cols = ['title', 'company', 'location']\n",
    "nlp.remove_stopwords_columns(df, text_cols, additionals, exceptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3d1134-6300-4bd4-abf8-cac16ab542af",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now let's __find the most frequent bigrams and trigrams__ for each column.\n",
    "\n",
    "Once again, we will __define a third NLP class to store our new implemented methods__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f51377e-0e9f-4a0b-8503-e7f015886c37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 unigrams of column 'title':\n",
      "[('manager', 1649), ('officer', 1274), ('assistant', 1045), ('support', 590), ('sales', 515), ('engineer', 430), ('coordinator', 423), ('administration', 422), ('senior', 410), ('accountant', 350)]\n",
      "\n",
      "Top 10 bigrams of column 'title':\n",
      "[(('property', 'manager'), 199), (('support', 'officer'), 167), (('general', 'manager'), 153), (('administration', 'officer'), 142), (('administration', 'assistant'), 141), (('people', 'culture'), 138), (('business', 'partner'), 138), (('customer', 'service'), 119), (('part', 'time'), 116), (('human', 'resources'), 115)]\n",
      "\n",
      "Top 10 trigrams of column 'title':\n",
      "[(('chief', 'executive', 'officer'), 77), (('accounts', 'payable', 'officer'), 73), (('hr', 'business', 'partner'), 46), (('business', 'development', 'manager'), 45), (('it', 'support', 'officer'), 44), (('real', 'estate', 'sales'), 44), (('administration', 'assistant', 'administration'), 36), (('retail', 'sales', 'assistant'), 36), (('property', 'manager', 'property'), 33), (('chief', 'financial', 'officer'), 33)]\n",
      "\n",
      "-----------------------------------\n",
      "Top 10 unigrams of column 'company':\n",
      "[('ltd', 1404), ('pty', 1330), ('australia', 678), ('group', 628), ('recruitment', 468), ('health', 348), ('private', 332), ('services', 330), ('advertiser', 286), ('limited', 270)]\n",
      "\n",
      "Top 10 bigrams of column 'company':\n",
      "[(('pty', 'ltd'), 1242), (('private', 'advertiser'), 286), (('australia', 'pty'), 203), (('real', 'estate'), 110), (('pty', 'limited'), 84), (('group', 'pty'), 80), (('sharp', 'carter'), 68), (('city', 'council'), 66), (('services', 'pty'), 60), (('ltd', 'private'), 42)]\n",
      "\n",
      "Top 10 trigrams of column 'company':\n",
      "[(('australia', 'pty', 'ltd'), 174), (('group', 'pty', 'ltd'), 56), (('services', 'pty', 'ltd'), 55), (('pty', 'ltd', 'private'), 42), (('ltd', 'private', 'advertiser'), 42), (('recruitment', 'pty', 'ltd'), 36), (('australian', 'federal', 'police'), 34), (('pty', 'ltd', 'australian'), 31), (('australia', 'pty', 'limited'), 28), (('recruitment', 'real', 'estate'), 27)]\n",
      "\n",
      "-----------------------------------\n",
      "Top 10 unigrams of column 'location':\n",
      "[('nsw', 3130), ('qld', 2421), ('sydney', 2287), ('vic', 2221), ('melbourne', 1793), ('brisbane', 1372), ('coast', 1130), ('wa', 904), ('perth', 778), ('north', 566)]\n",
      "\n",
      "Top 10 bigrams of column 'location':\n",
      "[(('sydney', 'nsw'), 2226), (('melbourne', 'vic'), 1718), (('brisbane', 'qld'), 1335), (('perth', 'wa'), 723), (('coast', 'qld'), 593), (('adelaide', 'sa'), 427), (('coast', 'nsw'), 349), (('gold', 'coast'), 314), (('nsw', 'sydney'), 264), (('newcastle', 'maitland'), 237)]\n",
      "\n",
      "Top 10 trigrams of column 'location':\n",
      "[(('gold', 'coast', 'qld'), 314), (('nsw', 'sydney', 'nsw'), 262), (('newcastle', 'maitland', 'hunter'), 237), (('maitland', 'hunter', 'nsw'), 237), (('sydney', 'nsw', 'sydney'), 216), (('qld', 'sydney', 'nsw'), 201), (('vic', 'sydney', 'nsw'), 192), (('nsw', 'melbourne', 'vic'), 188), (('nsw', 'brisbane', 'qld'), 183), (('sunshine', 'coast', 'qld'), 167)]\n",
      "\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "class NLP_ngrams():\n",
    "    def merge_documents_into_list(self, df, colname):\n",
    "        # store all column values into a list of strings\n",
    "        lst = []\n",
    "        for row in df[colname]:\n",
    "            if not isinstance(row, str): continue\n",
    "            lst += word_tokenize(row)\n",
    "        return lst\n",
    "    \n",
    "    def get_column_n_most_frequent_unigrams(self, df, colname, n):\n",
    "        lst = self.merge_documents_into_list(df, colname)\n",
    "        counts = {}\n",
    "        for word in lst: counts[word] = counts.get(word, 0) + 1\n",
    "        # return list of tuples (unigram, frequency) sorted by the frequency in decreasing order\n",
    "        return sorted(counts.items(), key=lambda x:x[1], reverse=True)[:n]\n",
    "        \n",
    "    def get_column_n_most_frequent_bigrams(self, df, colname, n, freq_filter=10):\n",
    "        lst = self.merge_documents_into_list(df, colname)\n",
    "        bcf = BigramCollocationFinder.from_words(lst)\n",
    "        bcf.apply_freq_filter(freq_filter) # filter bigrams that won't repeat at least 10 times\n",
    "        # return list of tuples (bigram, frequency) sorted by the frequency in decreasing order\n",
    "        return sorted(list(bcf.ngram_fd.items()), key=lambda x:x[1], reverse=True)[:n]\n",
    "    \n",
    "    def get_column_n_most_frequent_trigrams(self, df, colname, n, freq_filter=10):\n",
    "        lst = self.merge_documents_into_list(df, colname)\n",
    "        tcf = TrigramCollocationFinder.from_words(lst)\n",
    "        tcf.apply_freq_filter(freq_filter) # filter trigrams that won't repeat at least 10 times\n",
    "        # return list of tuples (trigram, frequency) sorted by the frequency in decreasing order\n",
    "        return sorted(list(tcf.ngram_fd.items()), key=lambda x:x[1], reverse=True)[:n]\n",
    "\n",
    "    def get_top_x_most_frequent_ngrams_of_column(self, df, colname, x):\n",
    "        # get the most frequent n-grams (uni, bi, and tri) within the column values\n",
    "        top_x_unigrams = self.get_column_n_most_frequent_unigrams(df, colname, x)\n",
    "        top_x_bigrams = self.get_column_n_most_frequent_bigrams(df, colname, x)\n",
    "        top_x_trigrams = self.get_column_n_most_frequent_trigrams(df, colname, x)\n",
    "        top_x_ngrams = [top_x_unigrams, top_x_bigrams, top_x_trigrams]\n",
    "        return top_x_ngrams\n",
    "    \n",
    "def get_top_x_most_frequent_ngrams_of_columns(df, colnames, x_cols):\n",
    "    ngram_names = {1:'unigrams', 2:'bigrams', 3:'trigrams'}\n",
    "    nlp = NLP_ngrams()\n",
    "    ngrams = {}\n",
    "    # loop through column names, display only top 10 most frequent n-grams, but save the top x ngrams passed as parameters\n",
    "    for i in range(len(colnames)):\n",
    "        # save top x most frequent ngrams of the column\n",
    "        column_ngrams = nlp.get_top_x_most_frequent_ngrams_of_column(df, colnames[i], x_cols[i])\n",
    "        \n",
    "        # save it in a dictionary (key = column name, value = list of lists of ngrams)\n",
    "        ngrams[colnames[i]] = column_ngrams\n",
    "        \n",
    "        # display only top 10 ngrams for each column\n",
    "        for j in range(1,4):\n",
    "            print(\"Top 10\", ngram_names[j], \"of column '\"+colnames[i]+\"':\")\n",
    "            print(column_ngrams[j-1][:10], end=\"\\n\\n\")\n",
    "        print(\"-\"*35)\n",
    "    \n",
    "    return ngrams # return the dictionary (keys = column names, values = list of ngrams)\n",
    "        \n",
    "colnames = ['title', 'company', 'location']\n",
    "xs = [200 for _ in range(len(text_cols))] # we will get top 200 of every column\n",
    "ngrams_per_column = get_top_x_most_frequent_ngrams_of_columns(df, colnames, xs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09d3ab-664d-47f1-85bf-2a67ae53bd37",
   "metadata": {},
   "source": [
    "We have seen some of the most frequent n-grams (unigrams, bigrams, and trigrams) for all our text columns.\n",
    "\n",
    "Let's __replace the each column values with the most common n-grams found for each column__.\n",
    "\n",
    "This will serve us as a method to standardize values and reduce the number of categorical unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2fe59c7-c4ad-475d-84ae-f7dd27bd3212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing column 'title':\n",
      "# Unique Values before replacement: 5398\n",
      "# Unique Values after replacement: 610\n",
      "\n",
      "Replacing column 'company':\n",
      "# Unique Values before replacement: 4925\n",
      "# Unique Values after replacement: 1732\n",
      "\n",
      "Replacing column 'location':\n",
      "# Unique Values before replacement: 1448\n",
      "# Unique Values after replacement: 87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NLP_replace_values():\n",
    "    def replace_column_values_based_on_ngrams(self, df, colname, ngrams):\n",
    "        # save all ngrams into a list for search purposes\n",
    "        list_unigrams = [[tuple_[0]] for tuple_ in ngrams[0]] \n",
    "        list_bigrams = [list(tuple_[0])  for tuple_ in ngrams[1]]\n",
    "        list_trigrams = [list(tuple_[0])  for tuple_ in ngrams[2]]\n",
    "        list_ngrams = [list_unigrams, list_bigrams, list_trigrams]\n",
    "            \n",
    "        # loop through column rows and replace the value with most common ngram\n",
    "        for i, value in enumerate(df[colname]):\n",
    "            # tokenize row value\n",
    "            if not isinstance(value, str): continue\n",
    "            tokens = word_tokenize(value)\n",
    "            \n",
    "            # save most frequent ngram, and its frequency for comparing purposes\n",
    "            highest_ngram, highest_ngram_frequency = \"\", -1\n",
    "            \n",
    "            # loop through ngrams (start by looking for trigrams within the tokenized words, if not found, search for bigrams, finally unigrams)\n",
    "            for j in range(2,-1,-1):\n",
    "                # list of unigrams, bigrams, or trigrams depending on iteration\n",
    "                j_ngrams = list_ngrams[j]\n",
    "                \n",
    "                # loop through tokenized words\n",
    "                for k in range(len(tokens) - j):\n",
    "                    # set current ngram, if ngram not found on the top most frequent, skip iteration\n",
    "                    ngram = tokens[k : k + j + 1]\n",
    "                    if ngram not in j_ngrams: continue\n",
    "                    \n",
    "                    # otherwise, if found, get the frequency of the ngram\n",
    "                    ngram_frequency = ngrams[j][j_ngrams.index(ngram)][1]\n",
    "                    \n",
    "                    # if the frequency is higher, replace values\n",
    "                    if ngram_frequency > highest_ngram_frequency:\n",
    "                        highest_ngram, highest_ngram_frequency = ngram, ngram_frequency\n",
    "                        \n",
    "                # if we found a trigram, we don't need to look for bigrams or unigrams\n",
    "                # if we found a bigram, we don't need to look for unigrams\n",
    "                if highest_ngram != \"\":\n",
    "                    break\n",
    "            # finally replace the row value with the frequent ngram identified\n",
    "            if highest_ngram != \"\":\n",
    "                df.loc[i, colname] = ' '.join(highest_ngram)\n",
    "    \n",
    "    def replace_columns_values_based_on_ngrams(self, df, colnames, ngrams_columns):\n",
    "        # apply value replacement based o ngrams for a set of columns\n",
    "        for colname in colnames:\n",
    "            print(\"Replacing column '\"+colname+\"':\")\n",
    "            print(\"# Unique Values before replacement:\", df[colname].nunique())\n",
    "            self.replace_column_values_based_on_ngrams(df, colname, ngrams_columns[colname])\n",
    "            print(\"# Unique Values after replacement:\", df[colname].nunique(), end=\"\\n\\n\")\n",
    "\n",
    "nlp = NLP_replace_values()\n",
    "text_cols = ['title', 'company', 'location']\n",
    "nlp.replace_columns_values_based_on_ngrams(df, text_cols, ngrams_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18a832a4-0825-4885-910b-067aef681c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>company_questions</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>link</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>support worker</td>\n",
       "      <td>ability gateway</td>\n",
       "      <td>$35.50 per hour [PPT]</td>\n",
       "      <td>wagga wagga wagga</td>\n",
       "      <td>Aged &amp; Disability Support</td>\n",
       "      <td>Part time</td>\n",
       "      <td>About usWe are an outcome focused NDIS service...</td>\n",
       "      <td>Do you own or have regular access to a car?Whi...</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73909631?type=prom...</td>\n",
       "      <td>Community Services &amp; Development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regional manager</td>\n",
       "      <td>tasmania</td>\n",
       "      <td>NaN</td>\n",
       "      <td>launceston north east</td>\n",
       "      <td>Child Welfare, Youth &amp; Family Services</td>\n",
       "      <td>Full time</td>\n",
       "      <td>CatholicCare Tasmania is the primary social se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>https://www.seek.com.au/job/73909232?type=prom...</td>\n",
       "      <td>Community Services &amp; Development</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              title          company                 salary  \\\n",
       "0    support worker  ability gateway  $35.50 per hour [PPT]   \n",
       "1  regional manager         tasmania                    NaN   \n",
       "\n",
       "                location                              department       type  \\\n",
       "0      wagga wagga wagga               Aged & Disability Support  Part time   \n",
       "1  launceston north east  Child Welfare, Youth & Family Services  Full time   \n",
       "\n",
       "                                         description  \\\n",
       "0  About usWe are an outcome focused NDIS service...   \n",
       "1  CatholicCare Tasmania is the primary social se...   \n",
       "\n",
       "                                   company_questions posted_date  \\\n",
       "0  Do you own or have regular access to a car?Whi...  2024-02-21   \n",
       "1                                                NaN  2024-02-21   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.seek.com.au/job/73909631?type=prom...   \n",
       "1  https://www.seek.com.au/job/73909232?type=prom...   \n",
       "\n",
       "                           industry  \n",
       "0  Community Services & Development  \n",
       "1  Community Services & Development  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b06239-1512-4005-a8d7-a143b8607173",
   "metadata": {},
   "source": [
    "Now our columns 'title', 'company', and 'location' are cleaned with reduced unique values.\n",
    "\n",
    "Let's proceed with __'department'__ that comes in the following format: __'field (industry)'__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "124de49d-e987-4eda-accb-ad5106a2477f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m splits[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# creating 2 new columns\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindustry\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepartment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(get_industry)\n\u001b[0;32m     18\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepartment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepartment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(get_department)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[44], line 8\u001b[0m, in \u001b[0;36mget_industry\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      6\u001b[0m matches \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m((.+?)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# some have 2 fields in it return just the first one\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matches[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# functions to extract the department and industry\n",
    "def get_industry(text):\n",
    "    # pattern to catch a group inside parenthesis\n",
    "    matches = re.findall('\\((.+?)\\)', text)\n",
    "    # some have 2 fields in it return just the first one\n",
    "    return matches[0]\n",
    "\n",
    "def get_department(text):\n",
    "    # pattern to catch a group inside parenthesis\n",
    "    splits = text.split('(')\n",
    "    # some have 2 fields in it return just the first one\n",
    "    return splits[0].strip()\n",
    "\n",
    "# creating 2 new columns\n",
    "df['industry'] = df['department'].apply(get_industry)\n",
    "df['department'] = df['department'].apply(get_department)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bbd8ca0-ef48-445f-b65c-3006f9261aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of industries: 26\n",
      "Number of departments: 206\n",
      "Industries:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "industry\n",
       "Accounting                                919\n",
       "Manufacturing, Transport & Logistics      732\n",
       "Administration & Office Support           648\n",
       "Information & Communication Technology    628\n",
       "Healthcare & Medical                      571\n",
       "Retail & Consumer Products                564\n",
       "Community Services & Development          562\n",
       "Real Estate & Property                    562\n",
       "Science & Technology                      556\n",
       "CEO & General Management                  556\n",
       "Human Resources & Recruitment             554\n",
       "Legal                                     552\n",
       "Insurance & Superannuation                552\n",
       "Sport & Recreation                        550\n",
       "Engineering                               550\n",
       "Marketing & Communications                302\n",
       "Sales                                     296\n",
       "Call Centre & Customer Service             61\n",
       "Farming, Animals & Conservation            26\n",
       "Trades & Services                          24\n",
       "Hospitality & Tourism                      11\n",
       "Education & Training                        8\n",
       "Construction                                7\n",
       "Government & Defence                        5\n",
       "Mining, Resources & Energy                  3\n",
       "Banking & Financial Services                1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of industries:\", df['industry'].nunique())\n",
    "print(\"Number of departments:\", df['department'].nunique())\n",
    "print(\"Industries:\")\n",
    "df['industry'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9979d-85b8-4d7e-ac88-15c08146a25c",
   "metadata": {},
   "source": [
    "Now we have extracted the industries of our jobs into a separate column.\n",
    "\n",
    "Let's proceed with the column __'posted_date'__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c1e3dfd-1100-4aa2-b301-756091b48014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_safe = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "123a138f-27d8-4802-a648-93c45a3bf83c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# casting posted_date as datetime\n",
    "df['posted_date'] = pd.to_datetime(df['posted_date'])\n",
    "\n",
    "def get_days_ago(date):\n",
    "    today = datetime.now()\n",
    "    return (today - date).days\n",
    "\n",
    "df['days_ago'] = df['posted_date'].apply(get_days_ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a7e5737c-7ab0-43ee-a5fc-a1ecc621f195",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'days_ago' unique values: 95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "days_ago\n",
       "22      2464\n",
       "23      2241\n",
       "24      1475\n",
       "27       787\n",
       "28       507\n",
       "        ... \n",
       "1641       1\n",
       "1281       1\n",
       "47         1\n",
       "531        1\n",
       "1791       1\n",
       "Name: count, Length: 95, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of 'days_ago' unique values:\", df['days_ago'].nunique())\n",
    "df['days_ago'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e618a55c-1899-4c47-9c34-2e7f5564e3cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "The __'posted_date'__ column has been processed into the __number of days elapsed from the posted date until today__.\n",
    "\n",
    "We can identify some outliers from the value counts, but we will get back to them on future steps.\n",
    "\n",
    "Let's proceed with __'type' column__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9956c0b-d055-4b07-b1ff-3867136869a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "Full time                                               7044\n",
       "Casual/Vacation                                          965\n",
       "Part time                                                959\n",
       "Contract/Temp                                            826\n",
       "Contract/Temp, Casual/Vacation, Full time, Part time       2\n",
       "Casual/Vacation, Full time                                 2\n",
       "Contract/Temp, Casual/Vacation, Part time                  1\n",
       "Contract/Temp, Part time                                   1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique values and counts of column 'type'\n",
    "df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f4cfc4-3d8c-4caf-9802-155cdac32b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def futherClean(df):\n",
    "    '''\n",
    "        function: futherClean -> cleans the data frame, perform actions like drop, changing the data type, mapping column and\n",
    "                                 getting number of days.\n",
    "        arg: df (pandas dataframe) -> given list\n",
    "        return: None\n",
    "    '''\n",
    "    df.drop(columns=['link'], inplace=True)\n",
    "    \n",
    "    df['posted_date'] = pd.to_datetime(df['posted_date'])\n",
    "    \n",
    "    dateToday = datetime.now()\n",
    "    df['daysAgo'] = (dateToday - df.posted_date).dt.days\n",
    "    \n",
    "    df['typeMapped'] = df.type.map({'Part time': 0, 'Contract/Temp, Casual/Vacation, Part time':0, 'Contract/Temp, Part time':0,\n",
    "                              'Full time': 1, 'Contract/Temp': 1, 'Contract/Temp':1, 'Casual/Vacation, Full time': 1, \n",
    "                               'Contract/Temp, Casual/Vacation, Full time, Part time': 1, 'Casual/Vacation': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2425437-b021-49a1-b85b-6674bc1a9251",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "futherClean(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa5095",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class salaryDistribution:    \n",
    "    def saprateTextAndNumbers(self, salary):\n",
    "        '''\n",
    "        function: saprateTextAndNumbers -> converts salary column (which contains values in text and numbers) into numbers and\n",
    "                                           creates the list that maps salary in per anum or per hour.\n",
    "        arg: salary (pandas series) -> salary column of dataframe\n",
    "        return: numCol (list) -> converts salary into list \n",
    "                perAnum (list) -> maps salary in per anum or per hour (p.a -> 1, p.h -> 0, NAN/empty List -> -1)\n",
    "        '''\n",
    "        numCol, perAnum = [], []\n",
    "        phList = ['p/h', 'hour', 'p.h', 'p/hr', '/h']\n",
    "        for i in salary:\n",
    "            if isinstance(i, str):\n",
    "                text = ''.join(re.findall(r'\\D+', i))\n",
    "                pattern = r'\\d{2,}(?:,\\d{3})*(?:\\.\\d{2,})?'   #r'\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?'\n",
    "                number = re.findall(pattern, i)\n",
    "                temp = re.findall(r'(\\d+)k', i)\n",
    "                if temp:\n",
    "                    number = [str(float(n) * 1000) for n in temp]\n",
    "                    temp.clear()\n",
    "                if ',' in i:\n",
    "                    number = [float(s.replace(',', '')) for s in number if isinstance(s, str)]\n",
    "                else:\n",
    "                    number = [float(s) for s in number if isinstance(s, str)]\n",
    "                    if any(True for s in number if s<1):\n",
    "                        number = [number[i] + number[i+1] for i in range(0,len(number)-1,2)]\n",
    "                flag = 0 if any(i in text for i in phList) else -1 if not number else 1 \n",
    "\n",
    "            else:\n",
    "                number = np.nan\n",
    "                flag = -1\n",
    "            numCol.append(number)\n",
    "            perAnum.append(flag)\n",
    "            number = flag = 0\n",
    "        return numCol, perAnum  \n",
    "    \n",
    "    def finalSalaryCalculation(self, perAnum, numCol, typeMapped):\n",
    "        '''\n",
    "        function: finalSalaryCalculation -> clauclates the final salary into per anum format.\n",
    "        arg: numCol (list) -> salary into list of numbers\n",
    "                perAnum (list) -> salary in per anum or per hour (p.a -> 1, p.h -> 0, NAN/empty List -> -1)\n",
    "        return: finalSalary (list) -> list of every salary into per anum format.\n",
    "        '''\n",
    "        finalSalary = []\n",
    "        AS = 0\n",
    "        for j, i, k in zip(perAnum, numCol, typeMapped):\n",
    "            if j != 0:\n",
    "                finalSalary.append(int(i) if isinstance(i, str) else i)\n",
    "            else:\n",
    "                Months = 12\n",
    "                AWH = 40 if k else 20\n",
    "                AS = [round(float(k) * AWH * Months, 2) for k in i]\n",
    "                finalSalary.append(AS)\n",
    "        return finalSalary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cfe6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "salaryDist = salaryDistribution()\n",
    "\n",
    "def salaryInt(finalSalary):\n",
    "    maxSalary, minSalary = [], []\n",
    "    for i in finalSalary:\n",
    "        if isinstance(i, float):\n",
    "            minSalary.append(np.nan)\n",
    "            maxSalary.append(np.nan)\n",
    "        else:\n",
    "            if len(i) > 1:\n",
    "                minSalary.append(int(i[0]))\n",
    "                maxSalary.append(int(i[1]))\n",
    "            elif len(i) == 1:\n",
    "                minSalary.append(int(i[0]))\n",
    "                maxSalary.append(int(i[0]))\n",
    "            else:\n",
    "                minSalary.append(np.nan)\n",
    "                maxSalary.append(np.nan) \n",
    "    return minSalary, maxSalary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905219ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['numCol'], df2['perAnum'] = salaryDist.saprateTextAndNumbers(df2.salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['finalSalary'] = salaryDist.finalSalaryCalculation(df2.perAnum, df2.numCol, df2.typeMapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32390110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['minSalary'], df2['maxSalary'] = salaryInt(df2.finalSalary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3550c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d90fc1",
   "metadata": {},
   "source": [
    "### Step 9. Unsupervised learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e68df",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = df2.copy()\n",
    "clean_data = clean_data[['title', 'company', 'location', 'daysAgo', 'typeMapped', 'minSalary', 'maxSalary']]\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f669a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a9f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Crea un modelo de KMeans para clusterizar los datos\n",
    "##Apply encoding to the categorical columns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "for col in clean_data.select_dtypes(include=['object']).columns:\n",
    "    clean_data[col + '_encoded'] = le.fit_transform(clean_data[col])\n",
    "\n",
    "\n",
    "df_econded = clean_data[['daysAgo', 'typeMapped', 'minSalary', 'maxSalary', 'title_encoded', 'company_encoded', 'location_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfce03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_econded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44bf50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check null values\n",
    "\n",
    "df_econded.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46461e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For now i will drop the null values\n",
    "df_econded = df_econded.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b90948",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_econded_scaled = scaler.fit_transform(df_econded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Elbow method to find the best number of clusters\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_elbow_method(df, max_clusters):\n",
    "    wcss = []\n",
    "    for i in range(1, max_clusters):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        kmeans.fit(df)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    plt.plot(range(1, max_clusters), wcss)\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()\n",
    "\n",
    "plot_elbow_method(df_econded_scaled, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0b690",
   "metadata": {},
   "source": [
    "Best clusterin K value is 4. We will use KMeans to cluster our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db37e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "### cluster k=3\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "kmeans.fit(df_econded_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1973086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a new column with the cluster number\n",
    "\n",
    "df_econded['cluster'] = kmeans.labels_\n",
    "df_econded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize the clusters\n",
    "\n",
    "def plot_clusters(df, x, y, hue):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df, x=x, y=y, hue=hue, palette='deep')\n",
    "    plt.title('Clusters')\n",
    "    plt.show()\n",
    "\n",
    "plot_clusters(df_econded, 'minSalary', 'maxSalary', 'cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411bdab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f7ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddbd960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead1d0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
